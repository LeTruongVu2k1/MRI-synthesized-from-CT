
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 23, 'patch_size': [256, 192], 'median_image_size_in_voxels': [251.5, 186.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Liver', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [30, 252, 186], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.2606104910373688, 'median': 0.24769647419452667, 'min': 0.0, 'percentile_00_5': 0.06384892016649246, 'percentile_99_5': 0.6531020402908325, 'std': 0.0987810268998146}}} 
 
2023-05-02 15:43:58.315858: unpacking dataset... 
2023-05-02 15:45:38.859222: unpacking done... 
2023-05-02 15:45:40.692692: do_dummy_2d_data_aug: False 
2023-05-02 15:45:44.017948: Using splits from existing split file: drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_Liver/splits_final.json 
2023-05-02 15:45:46.911243: The split file contains 5 splits. 
2023-05-02 15:45:48.787203: Desired fold for training: 1 
2023-05-02 15:45:50.808293: This split has 13 training and 3 validation cases. 
2023-05-02 15:46:23.962643: Unable to plot network architecture: 
2023-05-02 15:46:25.994265: No module named 'hiddenlayer' 
2023-05-02 15:46:33.030696:  
2023-05-02 15:46:34.967013: Epoch 24 
2023-05-02 15:46:36.846608: Current learning rate: 0.00781 
2023-05-02 15:50:41.309958: train_loss -0.9574 
2023-05-02 15:50:43.390365: val_loss -0.9084 
2023-05-02 15:50:45.375355: Pseudo dice [0.9504] 
2023-05-02 15:50:47.233067: Epoch time: 248.28 s 
2023-05-02 15:50:49.202555: Yayy! New best EMA pseudo Dice: 0.9416 
2023-05-02 15:51:11.941172:  
2023-05-02 15:51:13.715939: Epoch 25 
2023-05-02 15:51:16.264463: Current learning rate: 0.00772 
2023-05-02 15:53:49.428679: train_loss -0.957 
2023-05-02 15:53:51.487433: val_loss -0.9068 
2023-05-02 15:53:53.560097: Pseudo dice [0.9507] 
2023-05-02 15:53:56.313268: Epoch time: 157.49 s 
2023-05-02 15:53:59.114167: Yayy! New best EMA pseudo Dice: 0.9425 
2023-05-02 15:54:12.790224:  
2023-05-02 15:54:14.535233: Epoch 26 
2023-05-02 15:54:16.407918: Current learning rate: 0.00763 
2023-05-02 15:56:51.051977: train_loss -0.9556 
2023-05-02 15:56:53.176646: val_loss -0.8914 
2023-05-02 15:56:55.960987: Pseudo dice [0.9445] 
2023-05-02 15:56:58.023824: Epoch time: 158.26 s 
2023-05-02 15:56:59.994709: Yayy! New best EMA pseudo Dice: 0.9427 
2023-05-02 15:57:12.798609:  
2023-05-02 15:57:14.660060: Epoch 27 
2023-05-02 15:57:16.497319: Current learning rate: 0.00753 
2023-05-02 15:59:53.569791: train_loss -0.9569 
2023-05-02 15:59:55.520301: val_loss -0.9004 
2023-05-02 15:59:57.559712: Pseudo dice [0.9453] 
2023-05-02 16:00:00.445044: Epoch time: 160.77 s 
2023-05-02 16:00:03.180311: Yayy! New best EMA pseudo Dice: 0.9429 
2023-05-02 16:00:17.000544:  
2023-05-02 16:00:18.905308: Epoch 28 
2023-05-02 16:00:20.775123: Current learning rate: 0.00744 
2023-05-02 16:02:53.540120: train_loss -0.9579 
2023-05-02 16:02:55.549615: val_loss -0.893 
2023-05-02 16:02:57.577349: Pseudo dice [0.9434] 
2023-05-02 16:02:59.476523: Epoch time: 156.54 s 
2023-05-02 16:03:01.559266: Yayy! New best EMA pseudo Dice: 0.943 
2023-05-02 16:03:16.225463:  
2023-05-02 16:03:18.257414: Epoch 29 
2023-05-02 16:03:20.152947: Current learning rate: 0.00735 
2023-05-02 16:05:54.307854: train_loss -0.9607 
2023-05-02 16:05:56.342346: val_loss -0.9054 
2023-05-02 16:05:58.312218: Pseudo dice [0.9508] 
2023-05-02 16:06:00.335965: Epoch time: 158.09 s 
2023-05-02 16:06:08.684316: Yayy! New best EMA pseudo Dice: 0.9438 
2023-05-02 16:06:22.142242:  
2023-05-02 16:06:24.181641: Epoch 30 
2023-05-02 16:06:26.080159: Current learning rate: 0.00725 
2023-05-02 16:08:58.315464: train_loss -0.9627 
2023-05-02 16:09:00.171281: val_loss -0.9019 
2023-05-02 16:09:02.144178: Pseudo dice [0.9503] 
2023-05-02 16:09:04.182644: Epoch time: 156.17 s 
2023-05-02 16:09:06.082474: Yayy! New best EMA pseudo Dice: 0.9444 
2023-05-02 16:09:20.163960:  
2023-05-02 16:09:22.373701: Epoch 31 
2023-05-02 16:09:24.508842: Current learning rate: 0.00716 
2023-05-02 16:11:56.481694: train_loss -0.963 
2023-05-02 16:11:58.721914: val_loss -0.9068 
2023-05-02 16:12:01.408952: Pseudo dice [0.953] 
2023-05-02 16:12:03.400894: Epoch time: 156.32 s 
2023-05-02 16:12:05.318886: Yayy! New best EMA pseudo Dice: 0.9453 
2023-05-02 16:12:20.300632:  
2023-05-02 16:12:22.186002: Epoch 32 
2023-05-02 16:12:23.933855: Current learning rate: 0.00707 
2023-05-02 16:14:54.242707: train_loss -0.9607 
2023-05-02 16:14:56.053692: val_loss -0.8804 
2023-05-02 16:14:57.968361: Pseudo dice [0.9386] 
2023-05-02 16:14:59.798707: Epoch time: 153.94 s 
2023-05-02 16:15:06.522279:  
2023-05-02 16:15:08.490172: Epoch 33 
2023-05-02 16:15:11.527683: Current learning rate: 0.00697 
2023-05-02 16:17:41.366601: train_loss -0.9628 
2023-05-02 16:17:43.458702: val_loss -0.8896 
2023-05-02 16:17:45.574558: Pseudo dice [0.9451] 
2023-05-02 16:17:49.419703: Epoch time: 154.85 s 
2023-05-02 16:17:57.418773:  
2023-05-02 16:17:59.302437: Epoch 34 
2023-05-02 16:18:01.252550: Current learning rate: 0.00688 
2023-05-02 16:20:33.510012: train_loss -0.9652 
2023-05-02 16:20:35.378050: val_loss -0.8919 
2023-05-02 16:20:37.370998: Pseudo dice [0.9456] 
2023-05-02 16:20:40.175773: Epoch time: 156.09 s 
2023-05-02 16:20:48.039805:  
2023-05-02 16:20:51.816743: Epoch 35 
2023-05-02 16:20:54.057715: Current learning rate: 0.00679 
2023-05-02 16:23:24.998220: train_loss -0.9664 
2023-05-02 16:23:26.951789: val_loss -0.8867 
2023-05-02 16:23:28.957785: Pseudo dice [0.9461] 
2023-05-02 16:23:30.945205: Epoch time: 156.96 s 
2023-05-02 16:23:38.553265:  
2023-05-02 16:23:40.572434: Epoch 36 
2023-05-02 16:23:43.491016: Current learning rate: 0.00669 
2023-05-02 16:26:17.171552: train_loss -0.967 
2023-05-02 16:26:19.299012: val_loss -0.8952 
2023-05-02 16:26:21.030607: Pseudo dice [0.9491] 
2023-05-02 16:26:22.882175: Epoch time: 158.62 s 
2023-05-02 16:26:24.935707: Yayy! New best EMA pseudo Dice: 0.9453 
2023-05-02 16:26:39.690558:  
2023-05-02 16:26:41.725684: Epoch 37 
2023-05-02 16:26:44.974886: Current learning rate: 0.0066 
2023-05-02 16:29:17.591123: train_loss -0.9671 
2023-05-02 16:29:19.586810: val_loss -0.9112 
2023-05-02 16:29:21.637575: Pseudo dice [0.9548] 
2023-05-02 16:29:24.736795: Epoch time: 157.9 s 
2023-05-02 16:29:27.930769: Yayy! New best EMA pseudo Dice: 0.9463 
2023-05-02 16:29:41.707256:  
2023-05-02 16:29:43.833098: Epoch 38 
2023-05-02 16:29:45.672352: Current learning rate: 0.0065 
2023-05-02 16:32:16.202764: train_loss -0.9679 
2023-05-02 16:32:18.111945: val_loss -0.9007 
2023-05-02 16:32:20.183138: Pseudo dice [0.9506] 
2023-05-02 16:32:23.070158: Epoch time: 154.5 s 
2023-05-02 16:32:26.228149: Yayy! New best EMA pseudo Dice: 0.9467 
2023-05-02 16:32:40.539929:  
2023-05-02 16:32:42.594212: Epoch 39 
2023-05-02 16:32:45.374142: Current learning rate: 0.00641 
2023-05-02 16:35:19.001058: train_loss -0.9691 
2023-05-02 16:35:20.986997: val_loss -0.8964 
2023-05-02 16:35:23.206327: Pseudo dice [0.9479] 
2023-05-02 16:35:25.125627: Epoch time: 158.46 s 
2023-05-02 16:35:33.638232: Yayy! New best EMA pseudo Dice: 0.9468 
2023-05-02 16:35:46.735275:  
2023-05-02 16:35:48.925899: Epoch 40 
2023-05-02 16:35:50.983926: Current learning rate: 0.00631 
2023-05-02 16:38:22.639530: train_loss -0.9696 
2023-05-02 16:38:24.779465: val_loss -0.8987 
2023-05-02 16:38:27.701012: Pseudo dice [0.9499] 
2023-05-02 16:38:29.738854: Epoch time: 155.91 s 
2023-05-02 16:38:31.685966: Yayy! New best EMA pseudo Dice: 0.9471 
2023-05-02 16:38:46.428455:  
2023-05-02 16:38:48.475300: Epoch 41 
2023-05-02 16:38:51.132161: Current learning rate: 0.00622 
2023-05-02 16:41:22.607831: train_loss -0.9681 
2023-05-02 16:41:24.652067: val_loss -0.8856 
2023-05-02 16:41:26.543110: Pseudo dice [0.9439] 
2023-05-02 16:41:28.453879: Epoch time: 156.18 s 
2023-05-02 16:41:35.333182:  
2023-05-02 16:41:37.228832: Epoch 42 
2023-05-02 16:41:39.277707: Current learning rate: 0.00612 
2023-05-02 16:44:12.602570: train_loss -0.9689 
2023-05-02 16:44:14.646520: val_loss -0.8897 
2023-05-02 16:44:16.427959: Pseudo dice [0.9458] 
2023-05-02 16:44:18.501334: Epoch time: 157.27 s 
2023-05-02 16:44:26.824013:  
2023-05-02 16:44:28.720264: Epoch 43 
2023-05-02 16:44:30.781571: Current learning rate: 0.00603 
2023-05-02 16:47:02.927379: train_loss -0.9685 
2023-05-02 16:47:05.607041: val_loss -0.8956 
2023-05-02 16:47:07.437901: Pseudo dice [0.9485] 
2023-05-02 16:47:09.438079: Epoch time: 156.1 s 
2023-05-02 16:47:16.467024:  
2023-05-02 16:47:18.647739: Epoch 44 
2023-05-02 16:47:21.835444: Current learning rate: 0.00593 
2023-05-02 16:49:52.302798: train_loss -0.9696 
2023-05-02 16:49:54.497387: val_loss -0.8977 
2023-05-02 16:49:56.322594: Pseudo dice [0.9485] 
2023-05-02 16:49:58.406335: Epoch time: 155.84 s 
2023-05-02 16:50:05.504283:  
2023-05-02 16:50:08.694023: Epoch 45 
2023-05-02 16:50:10.579343: Current learning rate: 0.00584 
2023-05-02 16:52:39.999872: train_loss -0.9715 
2023-05-02 16:52:42.001567: val_loss -0.8851 
2023-05-02 16:52:44.041507: Pseudo dice [0.9452] 
2023-05-02 16:52:46.790927: Epoch time: 154.5 s 
2023-05-02 16:52:54.822616:  
2023-05-02 16:52:57.021896: Epoch 46 
2023-05-02 16:52:59.980241: Current learning rate: 0.00574 
2023-05-02 16:55:30.820209: train_loss -0.9714 
2023-05-02 16:55:32.806571: val_loss -0.9053 
2023-05-02 16:55:34.736695: Pseudo dice [0.9512] 
2023-05-02 16:55:37.776947: Epoch time: 156.0 s 
2023-05-02 16:55:40.073805: Yayy! New best EMA pseudo Dice: 0.9473 
2023-05-02 16:55:55.141615:  
2023-05-02 16:55:57.199765: Epoch 47 
2023-05-02 16:56:00.462069: Current learning rate: 0.00565 
2023-05-02 16:58:32.365539: train_loss -0.9715 
2023-05-02 16:58:34.560241: val_loss -0.8863 
2023-05-02 16:58:36.610220: Pseudo dice [0.9443] 
2023-05-02 16:58:39.559611: Epoch time: 157.22 s 
2023-05-02 16:58:46.484189:  
2023-05-02 16:58:48.439462: Epoch 48 
2023-05-02 16:58:50.412326: Current learning rate: 0.00555 
2023-05-02 17:01:22.846032: train_loss -0.9726 
2023-05-02 17:01:24.981046: val_loss -0.8941 
2023-05-02 17:01:27.116853: Pseudo dice [0.948] 
2023-05-02 17:01:29.084046: Epoch time: 156.36 s 
2023-05-02 17:01:37.101591:  
2023-05-02 17:01:39.092966: Epoch 49 
2023-05-02 17:01:42.104002: Current learning rate: 0.00546 
2023-05-02 17:04:10.640974: train_loss -0.9724 
2023-05-02 17:04:12.549903: val_loss -0.8856 
2023-05-02 17:04:14.481916: Pseudo dice [0.9434] 
2023-05-02 17:04:18.297821: Epoch time: 153.54 s 
2023-05-02 17:04:32.384231:  
2023-05-02 17:04:34.638930: Epoch 50 
2023-05-02 17:04:36.926908: Current learning rate: 0.00536 
2023-05-02 17:07:09.333468: train_loss -0.9732 
2023-05-02 17:07:11.399849: val_loss -0.8962 
2023-05-02 17:07:13.728770: Pseudo dice [0.9494] 
2023-05-02 17:07:15.521132: Epoch time: 156.95 s 
2023-05-02 17:07:24.502558:  
2023-05-02 17:07:26.361507: Epoch 51 
2023-05-02 17:07:29.622658: Current learning rate: 0.00526 
2023-05-02 17:09:59.171656: train_loss -0.9729 
2023-05-02 17:10:01.276751: val_loss -0.8972 
2023-05-02 17:10:03.345694: Pseudo dice [0.9488] 
2023-05-02 17:10:05.337200: Epoch time: 154.67 s 
2023-05-02 17:10:12.181139:  
2023-05-02 17:10:14.291528: Epoch 52 
2023-05-02 17:10:17.602001: Current learning rate: 0.00517 
2023-05-02 17:12:49.797928: train_loss -0.9731 
2023-05-02 17:12:51.925640: val_loss -0.8833 
2023-05-02 17:12:54.013018: Pseudo dice [0.9431] 
2023-05-02 17:12:56.062187: Epoch time: 157.62 s 
2023-05-02 17:13:02.961045:  
2023-05-02 17:13:04.781044: Epoch 53 
2023-05-02 17:13:06.819893: Current learning rate: 0.00507 
2023-05-02 17:15:38.135696: train_loss -0.9735 
2023-05-02 17:15:40.094134: val_loss -0.8972 
2023-05-02 17:15:42.070454: Pseudo dice [0.9494] 
2023-05-02 17:15:44.115978: Epoch time: 155.18 s 
2023-05-02 17:15:51.104677:  
2023-05-02 17:15:53.198745: Epoch 54 
2023-05-02 17:15:55.250137: Current learning rate: 0.00497 
2023-05-02 17:18:28.525968: train_loss -0.9739 
2023-05-02 17:18:30.640164: val_loss -0.89 
2023-05-02 17:18:33.605798: Pseudo dice [0.9464] 
2023-05-02 17:18:35.644165: Epoch time: 157.42 s 
2023-05-02 17:18:43.478338:  
2023-05-02 17:18:45.489569: Epoch 55 
2023-05-02 17:18:48.675981: Current learning rate: 0.00487 
2023-05-02 17:21:18.867834: train_loss -0.9742 
2023-05-02 17:21:20.923383: val_loss -0.8877 
2023-05-02 17:21:23.839939: Pseudo dice [0.9454] 
2023-05-02 17:21:25.686991: Epoch time: 155.39 s 
2023-05-02 17:21:35.510612:  
2023-05-02 17:21:37.413206: Epoch 56 
2023-05-02 17:21:39.444496: Current learning rate: 0.00478 
2023-05-02 17:24:08.486603: train_loss -0.974 
2023-05-02 17:24:10.374942: val_loss -0.8834 
2023-05-02 17:24:12.246986: Pseudo dice [0.9438] 
2023-05-02 17:24:14.293707: Epoch time: 152.98 s 
2023-05-02 17:24:20.985017:  
2023-05-02 17:24:23.183883: Epoch 57 
2023-05-02 17:24:25.215168: Current learning rate: 0.00468 
2023-05-02 17:26:56.458899: train_loss -0.9745 
2023-05-02 17:26:58.574528: val_loss -0.8955 
2023-05-02 17:27:00.573823: Pseudo dice [0.948] 
2023-05-02 17:27:02.582453: Epoch time: 155.47 s 
2023-05-02 17:27:11.221317:  
2023-05-02 17:27:13.313472: Epoch 58 
2023-05-02 17:27:15.314511: Current learning rate: 0.00458 
2023-05-02 17:29:46.997482: train_loss -0.9747 
2023-05-02 17:29:49.285361: val_loss -0.8886 
2023-05-02 17:29:51.231973: Pseudo dice [0.9462] 
2023-05-02 17:29:54.350271: Epoch time: 155.78 s 
2023-05-02 17:30:02.290925:  
2023-05-02 17:30:04.256217: Epoch 59 
2023-05-02 17:30:06.242802: Current learning rate: 0.00448 
2023-05-02 17:32:39.033915: train_loss -0.9749 
2023-05-02 17:32:41.097884: val_loss -0.8996 
2023-05-02 17:32:43.222050: Pseudo dice [0.9493] 
2023-05-02 17:32:46.275968: Epoch time: 156.74 s 
2023-05-02 17:33:00.322439:  
2023-05-02 17:33:02.457417: Epoch 60 
2023-05-02 17:33:04.580752: Current learning rate: 0.00438 
2023-05-02 17:35:34.925231: train_loss -0.9754 
2023-05-02 17:35:37.081902: val_loss -0.8963 
2023-05-02 17:35:39.177643: Pseudo dice [0.9487] 
2023-05-02 17:35:41.011838: Epoch time: 154.6 s 
2023-05-02 17:35:48.087560:  
2023-05-02 17:35:50.185054: Epoch 61 
2023-05-02 17:35:53.155144: Current learning rate: 0.00429 
2023-05-02 17:38:26.716166: train_loss -0.9762 
2023-05-02 17:38:28.860423: val_loss -0.8902 
2023-05-02 17:38:30.835968: Pseudo dice [0.9465] 
2023-05-02 17:38:33.635547: Epoch time: 158.63 s 
2023-05-02 17:38:42.467391:  
2023-05-02 17:38:44.464873: Epoch 62 
2023-05-02 17:38:47.150957: Current learning rate: 0.00419 
2023-05-02 17:41:19.004781: train_loss -0.9759 
2023-05-02 17:41:20.851498: val_loss -0.8902 
2023-05-02 17:41:22.830847: Pseudo dice [0.9462] 
2023-05-02 17:41:25.777056: Epoch time: 156.54 s 
2023-05-02 17:41:33.539641:  
2023-05-02 17:41:35.518957: Epoch 63 
2023-05-02 17:41:37.557970: Current learning rate: 0.00409 
2023-05-02 17:44:09.021143: train_loss -0.9761 
2023-05-02 17:44:11.209707: val_loss -0.8933 
2023-05-02 17:44:13.237255: Pseudo dice [0.9483] 
2023-05-02 17:44:15.985317: Epoch time: 155.48 s 
2023-05-02 17:44:23.843992:  
2023-05-02 17:44:25.851153: Epoch 64 
2023-05-02 17:44:27.746493: Current learning rate: 0.00399 
2023-05-02 17:47:00.909572: train_loss -0.976 
2023-05-02 17:47:02.892509: val_loss -0.8896 
2023-05-02 17:47:06.786708: Pseudo dice [0.9463] 
2023-05-02 17:47:08.861557: Epoch time: 157.07 s 
2023-05-02 17:47:15.594902:  
2023-05-02 17:47:17.653673: Epoch 65 
2023-05-02 17:47:20.260330: Current learning rate: 0.00389 
2023-05-02 17:49:50.157352: train_loss -0.9766 
2023-05-02 17:49:52.443250: val_loss -0.8899 
2023-05-02 17:49:54.964945: Pseudo dice [0.9477] 
2023-05-02 17:49:56.989262: Epoch time: 154.56 s 
2023-05-02 17:50:04.124301:  
2023-05-02 17:50:06.388366: Epoch 66 
2023-05-02 17:50:08.299350: Current learning rate: 0.00379 
2023-05-02 17:52:39.128021: train_loss -0.9768 
2023-05-02 17:52:41.157463: val_loss -0.9012 
2023-05-02 17:52:43.928126: Pseudo dice [0.9515] 
2023-05-02 17:52:47.392440: Epoch time: 155.01 s 
2023-05-02 17:52:50.606265: Yayy! New best EMA pseudo Dice: 0.9475 
2023-05-02 17:53:05.225780:  
2023-05-02 17:53:07.205860: Epoch 67 
2023-05-02 17:53:09.321791: Current learning rate: 0.00369 
2023-05-02 17:55:39.685930: train_loss -0.977 
2023-05-02 17:55:41.855421: val_loss -0.8902 
2023-05-02 17:55:43.994889: Pseudo dice [0.947] 
2023-05-02 17:55:48.845504: Epoch time: 154.46 s 
2023-05-02 17:55:57.077165:  
2023-05-02 17:55:59.166537: Epoch 68 
2023-05-02 17:56:01.760823: Current learning rate: 0.00359 
2023-05-02 17:58:33.021139: train_loss -0.9771 
2023-05-02 17:58:35.138511: val_loss -0.8871 
2023-05-02 17:58:37.142144: Pseudo dice [0.9475] 
2023-05-02 17:58:39.165457: Epoch time: 155.94 s 
2023-05-02 17:58:46.940710:  
2023-05-02 17:58:48.947031: Epoch 69 
2023-05-02 17:58:50.945892: Current learning rate: 0.00349 
2023-05-02 18:01:22.715907: train_loss -0.9775 
2023-05-02 18:01:24.613292: val_loss -0.8907 
2023-05-02 18:01:27.662457: Pseudo dice [0.9474] 
2023-05-02 18:01:30.684408: Epoch time: 155.78 s 
2023-05-02 18:01:45.802329:  
2023-05-02 18:01:47.900162: Epoch 70 
2023-05-02 18:01:51.156595: Current learning rate: 0.00338 
2023-05-02 18:04:23.872958: train_loss -0.9774 
2023-05-02 18:04:26.209511: val_loss -0.8979 
2023-05-02 18:04:28.414069: Pseudo dice [0.9503] 
2023-05-02 18:04:30.409076: Epoch time: 158.07 s 
2023-05-02 18:04:33.490501: Yayy! New best EMA pseudo Dice: 0.9477 
2023-05-02 18:04:48.129866:  
2023-05-02 18:04:50.262436: Epoch 71 
2023-05-02 18:04:52.322123: Current learning rate: 0.00328 
2023-05-02 18:07:24.491241: train_loss -0.9775 
2023-05-02 18:07:26.494869: val_loss -0.8893 
2023-05-02 18:07:28.788185: Pseudo dice [0.9459] 
2023-05-02 18:07:30.726400: Epoch time: 156.36 s 
2023-05-02 18:07:40.292146:  
2023-05-02 18:07:42.411539: Epoch 72 
2023-05-02 18:07:44.427610: Current learning rate: 0.00318 
2023-05-02 18:10:17.636685: train_loss -0.9779 
2023-05-02 18:10:19.891412: val_loss -0.8986 
2023-05-02 18:10:22.226776: Pseudo dice [0.9504] 
2023-05-02 18:10:24.878085: Epoch time: 157.35 s 
2023-05-02 18:10:26.930952: Yayy! New best EMA pseudo Dice: 0.9478 
2023-05-02 18:10:41.525238:  
2023-05-02 18:10:43.485658: Epoch 73 
2023-05-02 18:10:45.794492: Current learning rate: 0.00308 
2023-05-02 18:13:14.541825: train_loss -0.9781 
2023-05-02 18:13:16.508560: val_loss -0.8955 
2023-05-02 18:13:18.552571: Pseudo dice [0.9493] 
2023-05-02 18:13:20.625379: Epoch time: 153.02 s 
2023-05-02 18:13:22.479421: Yayy! New best EMA pseudo Dice: 0.948 
2023-05-02 18:13:36.344470:  
2023-05-02 18:13:38.382186: Epoch 74 
2023-05-02 18:13:41.174499: Current learning rate: 0.00297 
2023-05-02 18:16:11.158578: train_loss -0.9784 
2023-05-02 18:16:13.142701: val_loss -0.8879 
2023-05-02 18:16:15.163220: Pseudo dice [0.9457] 
2023-05-02 18:16:17.074777: Epoch time: 154.82 s 
2023-05-02 18:16:24.326927:  
2023-05-02 18:16:26.247765: Epoch 75 
2023-05-02 18:16:28.188494: Current learning rate: 0.00287 
2023-05-02 18:18:57.834305: train_loss -0.9783 
2023-05-02 18:18:59.776123: val_loss -0.8831 
2023-05-02 18:19:01.791045: Pseudo dice [0.944] 
2023-05-02 18:19:03.785101: Epoch time: 153.51 s 
2023-05-02 18:19:13.617013:  
2023-05-02 18:19:15.550422: Epoch 76 
2023-05-02 18:19:17.391771: Current learning rate: 0.00277 
2023-05-02 18:21:49.994653: train_loss -0.9777 
2023-05-02 18:21:51.976115: val_loss -0.8811 
2023-05-02 18:21:53.907271: Pseudo dice [0.943] 
2023-05-02 18:21:56.700777: Epoch time: 156.38 s 
2023-05-02 18:22:04.856483:  
2023-05-02 18:22:06.803235: Epoch 77 
2023-05-02 18:22:08.869222: Current learning rate: 0.00266 
2023-05-02 18:24:42.355337: train_loss -0.9784 
2023-05-02 18:24:44.416977: val_loss -0.898 
2023-05-02 18:24:46.567279: Pseudo dice [0.9502] 
2023-05-02 18:24:48.773301: Epoch time: 157.5 s 
2023-05-02 18:24:56.541327:  
2023-05-02 18:24:58.478862: Epoch 78 
2023-05-02 18:25:00.477137: Current learning rate: 0.00256 
2023-05-02 18:27:33.210721: train_loss -0.9783 
2023-05-02 18:27:35.202017: val_loss -0.8909 
2023-05-02 18:27:37.069934: Pseudo dice [0.9485] 
2023-05-02 18:27:39.008215: Epoch time: 156.67 s 
2023-05-02 18:27:47.011528:  
2023-05-02 18:27:49.068107: Epoch 79 
2023-05-02 18:27:51.076073: Current learning rate: 0.00245 
2023-05-02 18:30:22.034306: train_loss -0.9787 
2023-05-02 18:30:24.068012: val_loss -0.8831 
2023-05-02 18:30:26.231980: Pseudo dice [0.9464] 
2023-05-02 18:30:28.174748: Epoch time: 155.02 s 
2023-05-02 18:30:44.000316:  
2023-05-02 18:30:46.138095: Epoch 80 
2023-05-02 18:30:48.143109: Current learning rate: 0.00235 
2023-05-02 18:33:17.133571: train_loss -0.9787 
2023-05-02 18:33:19.321658: val_loss -0.8835 
2023-05-02 18:33:21.410515: Pseudo dice [0.945] 
2023-05-02 18:33:24.058024: Epoch time: 153.13 s 
2023-05-02 18:33:31.337558:  
2023-05-02 18:33:33.479313: Epoch 81 
2023-05-02 18:33:35.422936: Current learning rate: 0.00224 
2023-05-02 18:36:06.470582: train_loss -0.979 
2023-05-02 18:36:08.561948: val_loss -0.888 
2023-05-02 18:36:10.576407: Pseudo dice [0.9461] 
2023-05-02 18:36:12.675448: Epoch time: 155.13 s 
2023-05-02 18:36:21.442539:  
2023-05-02 18:36:26.441020: Epoch 82 
2023-05-02 18:36:29.157408: Current learning rate: 0.00214 
2023-05-02 18:38:59.113462: train_loss -0.9794 
2023-05-02 18:39:01.156716: val_loss -0.8917 
2023-05-02 18:39:04.375916: Pseudo dice [0.9472] 
2023-05-02 18:39:07.090990: Epoch time: 157.67 s 
2023-05-02 18:39:14.662309:  
2023-05-02 18:39:16.703938: Epoch 83 
2023-05-02 18:39:19.458141: Current learning rate: 0.00203 
2023-05-02 18:41:51.702355: train_loss -0.9785 
2023-05-02 18:41:53.796480: val_loss -0.8839 
2023-05-02 18:41:55.770064: Pseudo dice [0.9465] 
2023-05-02 18:41:57.781133: Epoch time: 157.04 s 
2023-05-02 18:42:04.601768:  
2023-05-02 18:42:06.781924: Epoch 84 
2023-05-02 18:42:09.863696: Current learning rate: 0.00192 
2023-05-02 18:44:39.009820: train_loss -0.9797 
2023-05-02 18:44:40.934574: val_loss -0.8819 
2023-05-02 18:44:43.614482: Pseudo dice [0.943] 
2023-05-02 18:44:46.531816: Epoch time: 154.41 s 
2023-05-02 18:44:54.065692:  
2023-05-02 18:44:56.099443: Epoch 85 
2023-05-02 18:44:57.940253: Current learning rate: 0.00181 
2023-05-02 18:47:28.452759: train_loss -0.9795 
2023-05-02 18:47:30.737594: val_loss -0.8829 
2023-05-02 18:47:32.813462: Pseudo dice [0.9437] 
2023-05-02 18:47:35.986236: Epoch time: 154.39 s 
2023-05-02 18:47:43.863276:  
2023-05-02 18:47:45.909943: Epoch 86 
2023-05-02 18:47:48.002656: Current learning rate: 0.0017 
2023-05-02 18:50:18.009340: train_loss -0.9796 
2023-05-02 18:50:19.935866: val_loss -0.881 
2023-05-02 18:50:22.933893: Pseudo dice [0.9435] 
2023-05-02 18:50:26.124394: Epoch time: 154.15 s 
2023-05-02 18:50:33.156908:  
2023-05-02 18:50:35.240449: Epoch 87 
2023-05-02 18:50:38.596930: Current learning rate: 0.00159 
2023-05-02 18:53:09.148257: train_loss -0.9799 
2023-05-02 18:53:11.116518: val_loss -0.8891 
2023-05-02 18:53:14.088961: Pseudo dice [0.9453] 
2023-05-02 18:53:16.011707: Epoch time: 155.99 s 
2023-05-02 18:53:22.910615:  
2023-05-02 18:53:24.649844: Epoch 88 
2023-05-02 18:53:27.293159: Current learning rate: 0.00148 
2023-05-02 18:55:57.742318: train_loss -0.9799 
2023-05-02 18:55:59.885239: val_loss -0.8933 
2023-05-02 18:56:02.619524: Pseudo dice [0.9489] 
2023-05-02 18:56:04.783149: Epoch time: 154.83 s 
2023-05-02 18:56:11.438042:  
2023-05-02 18:56:13.439255: Epoch 89 
2023-05-02 18:56:16.434131: Current learning rate: 0.00137 
2023-05-02 18:58:47.106303: train_loss -0.98 
2023-05-02 18:58:49.023091: val_loss -0.888 
2023-05-02 18:58:51.723108: Pseudo dice [0.9472] 
2023-05-02 18:58:54.782308: Epoch time: 155.67 s 
2023-05-02 18:59:08.157891:  
2023-05-02 18:59:10.228920: Epoch 90 
2023-05-02 18:59:13.156041: Current learning rate: 0.00126 
2023-05-02 19:01:43.788487: train_loss -0.9793 
2023-05-02 19:01:45.987657: val_loss -0.8832 
2023-05-02 19:01:48.247666: Pseudo dice [0.9444] 
2023-05-02 19:01:50.358362: Epoch time: 155.63 s 
2023-05-02 19:01:58.206702:  
2023-05-02 19:02:00.102801: Epoch 91 
2023-05-02 19:02:02.979886: Current learning rate: 0.00115 
2023-05-02 19:04:31.604416: train_loss -0.9801 
2023-05-02 19:04:43.568282: val_loss -0.8783 
2023-05-02 19:04:47.376490: Pseudo dice [0.9438] 
2023-05-02 19:04:50.469734: Epoch time: 153.4 s 
2023-05-02 19:04:57.103951:  
2023-05-02 19:04:58.966739: Epoch 92 
2023-05-02 19:05:01.140366: Current learning rate: 0.00103 
2023-05-02 19:07:29.367326: train_loss -0.9802 
2023-05-02 19:07:31.316849: val_loss -0.885 
2023-05-02 19:07:33.337049: Pseudo dice [0.9448] 
2023-05-02 19:07:35.838507: Epoch time: 152.27 s 
2023-05-02 19:07:43.408846:  
2023-05-02 19:07:45.261864: Epoch 93 
2023-05-02 19:07:49.948921: Current learning rate: 0.00091 
2023-05-02 19:10:20.380619: train_loss -0.9802 
2023-05-02 19:10:22.571772: val_loss -0.8767 
2023-05-02 19:10:25.631718: Pseudo dice [0.9437] 
2023-05-02 19:10:27.426390: Epoch time: 156.97 s 
2023-05-02 19:10:35.242362:  
2023-05-02 19:10:37.355998: Epoch 94 
2023-05-02 19:10:39.430478: Current learning rate: 0.00079 
2023-05-02 19:13:10.183612: train_loss -0.9805 
2023-05-02 19:13:12.013578: val_loss -0.8779 
2023-05-02 19:13:15.326948: Pseudo dice [0.9421] 
2023-05-02 19:13:17.315741: Epoch time: 154.94 s 
2023-05-02 19:13:23.992958:  
2023-05-02 19:13:26.116189: Epoch 95 
2023-05-02 19:13:28.597567: Current learning rate: 0.00067 
2023-05-02 19:15:59.288775: train_loss -0.9806 
2023-05-02 19:16:01.416522: val_loss -0.8837 
2023-05-02 19:16:03.601472: Pseudo dice [0.9448] 
2023-05-02 19:16:05.685757: Epoch time: 155.3 s 
2023-05-02 19:16:12.548938:  
2023-05-02 19:16:14.555899: Epoch 96 
2023-05-02 19:16:17.654878: Current learning rate: 0.00055 
2023-05-02 19:18:49.370189: train_loss -0.9806 
2023-05-02 19:18:51.334024: val_loss -0.8847 
2023-05-02 19:18:53.473267: Pseudo dice [0.9469] 
2023-05-02 19:18:55.442679: Epoch time: 156.82 s 
2023-05-02 19:19:03.377274:  
2023-05-02 19:19:05.116559: Epoch 97 
2023-05-02 19:19:07.023257: Current learning rate: 0.00043 
2023-05-02 19:21:37.108615: train_loss -0.9805 
2023-05-02 19:21:39.310532: val_loss -0.8844 
2023-05-02 19:21:41.188035: Pseudo dice [0.9441] 
2023-05-02 19:21:42.994370: Epoch time: 153.73 s 
2023-05-02 19:21:50.210810:  
2023-05-02 19:21:52.004282: Epoch 98 
2023-05-02 19:21:54.584144: Current learning rate: 0.0003 
2023-05-02 19:24:26.666336: train_loss -0.9804 
2023-05-02 19:24:28.650997: val_loss -0.8788 
2023-05-02 19:24:31.571242: Pseudo dice [0.9437] 
2023-05-02 19:24:33.500415: Epoch time: 156.46 s 
2023-05-02 19:24:41.619517:  
2023-05-02 19:24:43.366866: Epoch 99 
2023-05-02 19:24:46.136962: Current learning rate: 0.00016 
2023-05-02 19:27:18.327497: train_loss -0.9805 
2023-05-02 19:27:20.380322: val_loss -0.8863 
2023-05-02 19:27:23.231529: Pseudo dice [0.9461] 
2023-05-02 19:27:25.016181: Epoch time: 156.71 s 
2023-05-02 19:27:40.238971: Training done. 
2023-05-02 19:27:42.784148: Using splits from existing split file: drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_Liver/splits_final.json 
2023-05-02 19:27:44.602121: The split file contains 5 splits. 
2023-05-02 19:27:46.559635: Desired fold for training: 1 
2023-05-02 19:27:48.863174: This split has 13 training and 3 validation cases. 
2023-05-02 19:27:50.789887: predicting 19 
2023-05-02 19:28:06.673595: predicting 22 
2023-05-02 19:28:10.622769: predicting 37 
2023-05-02 19:29:08.701818: Validation complete 
2023-05-02 19:29:10.663131: Mean Validation Dice:  0.9443299627693807 
