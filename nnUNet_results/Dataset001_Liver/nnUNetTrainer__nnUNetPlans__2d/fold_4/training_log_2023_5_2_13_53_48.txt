
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 23, 'patch_size': [256, 192], 'median_image_size_in_voxels': [251.5, 186.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Liver', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [30, 252, 186], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.2606104910373688, 'median': 0.24769647419452667, 'min': 0.0, 'percentile_00_5': 0.06384892016649246, 'percentile_99_5': 0.6531020402908325, 'std': 0.0987810268998146}}} 
 
2023-05-02 13:54:04.407991: unpacking dataset... 
2023-05-02 13:55:22.802614: unpacking done... 
2023-05-02 13:55:23.999365: do_dummy_2d_data_aug: False 
2023-05-02 13:55:27.166312: Using splits from existing split file: drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_Liver/splits_final.json 
2023-05-02 13:55:29.255642: The split file contains 5 splits. 
2023-05-02 13:55:30.548252: Desired fold for training: 4 
2023-05-02 13:55:32.551186: This split has 13 training and 3 validation cases. 
2023-05-02 13:55:50.360765: Unable to plot network architecture: 
2023-05-02 13:55:51.735131: No module named 'hiddenlayer' 
2023-05-02 13:55:54.972536:  
2023-05-02 13:55:56.246374: Epoch 24 
2023-05-02 13:55:57.573913: Current learning rate: 0.00781 
2023-05-02 13:59:22.342526: train_loss -0.9563 
2023-05-02 13:59:23.920363: val_loss -0.3434 
2023-05-02 13:59:25.364630: Pseudo dice [0.7164] 
2023-05-02 13:59:26.525372: Epoch time: 207.37 s 
2023-05-02 13:59:28.934327: Yayy! New best EMA pseudo Dice: 0.7097 
2023-05-02 13:59:43.409248:  
2023-05-02 13:59:44.734126: Epoch 25 
2023-05-02 13:59:45.823780: Current learning rate: 0.00772 
2023-05-02 14:02:04.427520: train_loss -0.9521 
2023-05-02 14:02:05.798671: val_loss -0.2815 
2023-05-02 14:02:07.117609: Pseudo dice [0.6849] 
2023-05-02 14:02:10.293052: Epoch time: 141.02 s 
2023-05-02 14:02:15.821524:  
2023-05-02 14:02:17.161107: Epoch 26 
2023-05-02 14:02:19.345470: Current learning rate: 0.00763 
2023-05-02 14:04:36.595710: train_loss -0.9567 
2023-05-02 14:04:38.375555: val_loss -0.3279 
2023-05-02 14:04:41.161801: Pseudo dice [0.7072] 
2023-05-02 14:04:42.515151: Epoch time: 140.78 s 
2023-05-02 14:04:47.970353:  
2023-05-02 14:04:49.404268: Epoch 27 
2023-05-02 14:04:51.946223: Current learning rate: 0.00753 
2023-05-02 14:07:11.296955: train_loss -0.9579 
2023-05-02 14:07:12.553285: val_loss -0.3059 
2023-05-02 14:07:14.715337: Pseudo dice [0.7124] 
2023-05-02 14:07:16.703770: Epoch time: 143.33 s 
2023-05-02 14:07:22.641932:  
2023-05-02 14:07:24.170874: Epoch 28 
2023-05-02 14:07:25.447315: Current learning rate: 0.00744 
2023-05-02 14:09:45.203225: train_loss -0.9598 
2023-05-02 14:09:46.772197: val_loss -0.3461 
2023-05-02 14:09:48.855686: Pseudo dice [0.7264] 
2023-05-02 14:09:50.280007: Epoch time: 142.56 s 
2023-05-02 14:09:56.804843:  
2023-05-02 14:09:58.305606: Epoch 29 
2023-05-02 14:09:59.629798: Current learning rate: 0.00735 
2023-05-02 14:12:17.491505: train_loss -0.9608 
2023-05-02 14:12:19.143861: val_loss -0.4069 
2023-05-02 14:12:21.873953: Pseudo dice [0.7423] 
2023-05-02 14:12:23.994540: Epoch time: 140.69 s 
2023-05-02 14:12:29.755012: Yayy! New best EMA pseudo Dice: 0.7128 
2023-05-02 14:12:39.798536:  
2023-05-02 14:12:41.392221: Epoch 30 
2023-05-02 14:12:42.688189: Current learning rate: 0.00725 
2023-05-02 14:15:02.208901: train_loss -0.9599 
2023-05-02 14:15:03.595042: val_loss -0.3559 
2023-05-02 14:15:05.770411: Pseudo dice [0.7212] 
2023-05-02 14:15:06.988120: Epoch time: 142.41 s 
2023-05-02 14:15:08.419523: Yayy! New best EMA pseudo Dice: 0.7137 
2023-05-02 14:15:19.492545:  
2023-05-02 14:15:20.850908: Epoch 31 
2023-05-02 14:15:22.139624: Current learning rate: 0.00716 
2023-05-02 14:17:41.569605: train_loss -0.9624 
2023-05-02 14:17:43.056217: val_loss -0.3366 
2023-05-02 14:17:45.846524: Pseudo dice [0.7169] 
2023-05-02 14:17:48.124573: Epoch time: 142.08 s 
2023-05-02 14:17:49.416497: Yayy! New best EMA pseudo Dice: 0.714 
2023-05-02 14:17:59.801241:  
2023-05-02 14:18:01.208325: Epoch 32 
2023-05-02 14:18:04.114514: Current learning rate: 0.00707 
2023-05-02 14:20:22.350831: train_loss -0.9642 
2023-05-02 14:20:24.079520: val_loss -0.3505 
2023-05-02 14:20:25.412601: Pseudo dice [0.7186] 
2023-05-02 14:20:27.723104: Epoch time: 142.55 s 
2023-05-02 14:20:30.096837: Yayy! New best EMA pseudo Dice: 0.7145 
2023-05-02 14:20:39.864467:  
2023-05-02 14:20:42.157743: Epoch 33 
2023-05-02 14:20:43.848455: Current learning rate: 0.00697 
2023-05-02 14:23:02.795465: train_loss -0.9633 
2023-05-02 14:23:04.252742: val_loss -0.3179 
2023-05-02 14:23:05.967041: Pseudo dice [0.7006] 
2023-05-02 14:23:08.878211: Epoch time: 142.93 s 
2023-05-02 14:23:15.336952:  
2023-05-02 14:23:16.687334: Epoch 34 
2023-05-02 14:23:19.175970: Current learning rate: 0.00688 
2023-05-02 14:25:36.958588: train_loss -0.9649 
2023-05-02 14:25:38.595336: val_loss -0.3419 
2023-05-02 14:25:40.901503: Pseudo dice [0.7172] 
2023-05-02 14:25:42.195242: Epoch time: 141.62 s 
2023-05-02 14:25:48.580637:  
2023-05-02 14:25:50.059367: Epoch 35 
2023-05-02 14:25:51.477654: Current learning rate: 0.00679 
2023-05-02 14:28:10.925133: train_loss -0.9652 
2023-05-02 14:28:12.344507: val_loss -0.3039 
2023-05-02 14:28:13.694026: Pseudo dice [0.6991] 
2023-05-02 14:28:15.081284: Epoch time: 142.35 s 
2023-05-02 14:28:21.835604:  
2023-05-02 14:28:23.279463: Epoch 36 
2023-05-02 14:28:25.559243: Current learning rate: 0.00669 
2023-05-02 14:30:45.421818: train_loss -0.9669 
2023-05-02 14:30:46.956438: val_loss -0.3649 
2023-05-02 14:30:48.798186: Pseudo dice [0.7242] 
2023-05-02 14:30:50.340548: Epoch time: 143.59 s 
2023-05-02 14:30:56.642215:  
2023-05-02 14:30:58.242108: Epoch 37 
2023-05-02 14:31:00.596581: Current learning rate: 0.0066 
2023-05-02 14:33:20.033551: train_loss -0.9679 
2023-05-02 14:33:21.451843: val_loss -0.3448 
2023-05-02 14:33:23.821132: Pseudo dice [0.7184] 
2023-05-02 14:33:26.213131: Epoch time: 143.39 s 
2023-05-02 14:33:32.700703:  
2023-05-02 14:33:34.232316: Epoch 38 
2023-05-02 14:33:35.717142: Current learning rate: 0.0065 
2023-05-02 14:35:54.971784: train_loss -0.9683 
2023-05-02 14:35:56.514362: val_loss -0.3434 
2023-05-02 14:35:59.193175: Pseudo dice [0.7254] 
2023-05-02 14:36:01.753067: Epoch time: 142.27 s 
2023-05-02 14:36:03.157352: Yayy! New best EMA pseudo Dice: 0.7149 
2023-05-02 14:36:13.742334:  
2023-05-02 14:36:15.136645: Epoch 39 
2023-05-02 14:36:16.565742: Current learning rate: 0.00641 
2023-05-02 14:38:35.812169: train_loss -0.9689 
2023-05-02 14:38:37.366946: val_loss -0.3205 
2023-05-02 14:38:40.193333: Pseudo dice [0.7004] 
2023-05-02 14:38:42.362407: Epoch time: 142.07 s 
2023-05-02 14:38:52.988056:  
2023-05-02 14:38:54.575375: Epoch 40 
2023-05-02 14:38:57.283865: Current learning rate: 0.00631 
2023-05-02 14:41:15.184532: train_loss -0.9685 
2023-05-02 14:41:16.530355: val_loss -0.3361 
2023-05-02 14:41:17.879652: Pseudo dice [0.7181] 
2023-05-02 14:41:19.313334: Epoch time: 142.2 s 
2023-05-02 14:41:24.437110:  
2023-05-02 14:41:26.094223: Epoch 41 
2023-05-02 14:41:28.110640: Current learning rate: 0.00622 
2023-05-02 14:43:46.133116: train_loss -0.9696 
2023-05-02 14:43:47.541558: val_loss -0.3225 
2023-05-02 14:43:50.476637: Pseudo dice [0.709] 
2023-05-02 14:43:52.770918: Epoch time: 141.7 s 
2023-05-02 14:43:57.818908:  
2023-05-02 14:43:59.339797: Epoch 42 
2023-05-02 14:44:02.006679: Current learning rate: 0.00612 
2023-05-02 14:46:18.419667: train_loss -0.9701 
2023-05-02 14:46:19.853018: val_loss -0.3458 
2023-05-02 14:46:23.584003: Pseudo dice [0.7155] 
2023-05-02 14:46:25.738169: Epoch time: 140.6 s 
2023-05-02 14:46:30.703529:  
2023-05-02 14:46:32.213318: Epoch 43 
2023-05-02 14:46:33.728274: Current learning rate: 0.00603 
2023-05-02 14:48:52.308237: train_loss -0.9703 
2023-05-02 14:48:53.655640: val_loss -0.3148 
2023-05-02 14:48:56.152395: Pseudo dice [0.7102] 
2023-05-02 14:48:57.658349: Epoch time: 141.61 s 
2023-05-02 14:49:03.145138:  
2023-05-02 14:49:04.397876: Epoch 44 
2023-05-02 14:49:06.977750: Current learning rate: 0.00593 
2023-05-02 14:51:24.392226: train_loss -0.97 
2023-05-02 14:51:25.731174: val_loss -0.3196 
2023-05-02 14:51:28.478279: Pseudo dice [0.7155] 
2023-05-02 14:51:30.844435: Epoch time: 141.25 s 
2023-05-02 14:51:36.663605:  
2023-05-02 14:51:37.981524: Epoch 45 
2023-05-02 14:51:40.327970: Current learning rate: 0.00584 
2023-05-02 14:53:57.996179: train_loss -0.9707 
2023-05-02 14:53:59.566584: val_loss -0.3228 
2023-05-02 14:54:02.129713: Pseudo dice [0.7093] 
2023-05-02 14:54:03.453696: Epoch time: 141.33 s 
2023-05-02 14:54:09.468590:  
2023-05-02 14:54:11.024608: Epoch 46 
2023-05-02 14:54:13.894648: Current learning rate: 0.00574 
2023-05-02 14:56:31.086672: train_loss -0.9719 
2023-05-02 14:56:32.453640: val_loss -0.371 
2023-05-02 14:56:34.832728: Pseudo dice [0.7284] 
2023-05-02 14:56:36.084622: Epoch time: 141.62 s 
2023-05-02 14:56:42.180859:  
2023-05-02 14:56:43.493015: Epoch 47 
2023-05-02 14:56:44.783942: Current learning rate: 0.00565 
2023-05-02 14:59:03.038181: train_loss -0.9721 
2023-05-02 14:59:04.337420: val_loss -0.2538 
2023-05-02 14:59:06.710603: Pseudo dice [0.6817] 
2023-05-02 14:59:08.698931: Epoch time: 140.86 s 
2023-05-02 14:59:15.387959:  
2023-05-02 14:59:16.728474: Epoch 48 
2023-05-02 14:59:19.483492: Current learning rate: 0.00555 
2023-05-02 15:01:36.781155: train_loss -0.9717 
2023-05-02 15:01:38.140602: val_loss -0.324 
2023-05-02 15:01:39.546252: Pseudo dice [0.71] 
2023-05-02 15:01:40.870262: Epoch time: 141.39 s 
2023-05-02 15:01:46.460867:  
2023-05-02 15:01:47.920771: Epoch 49 
2023-05-02 15:01:50.408231: Current learning rate: 0.00546 
2023-05-02 15:04:08.202677: train_loss -0.9722 
2023-05-02 15:04:09.769551: val_loss -0.3542 
2023-05-02 15:04:12.269223: Pseudo dice [0.7258] 
2023-05-02 15:04:13.635747: Epoch time: 141.74 s 
2023-05-02 15:04:22.905809:  
2023-05-02 15:04:24.365894: Epoch 50 
2023-05-02 15:04:26.901241: Current learning rate: 0.00536 
2023-05-02 15:06:44.493558: train_loss -0.9727 
2023-05-02 15:06:46.018913: val_loss -0.3148 
2023-05-02 15:06:47.398925: Pseudo dice [0.7052] 
2023-05-02 15:06:49.445149: Epoch time: 141.59 s 
2023-05-02 15:06:58.123882:  
2023-05-02 15:06:59.458387: Epoch 51 
2023-05-02 15:07:01.585539: Current learning rate: 0.00526 
2023-05-02 15:09:22.575734: train_loss -0.973 
2023-05-02 15:09:23.953600: val_loss -0.3192 
2023-05-02 15:09:26.284001: Pseudo dice [0.7099] 
2023-05-02 15:09:27.863748: Epoch time: 144.45 s 
2023-05-02 15:09:33.961261:  
2023-05-02 15:09:35.265034: Epoch 52 
2023-05-02 15:09:37.852660: Current learning rate: 0.00517 
2023-05-02 15:11:57.134645: train_loss -0.9736 
2023-05-02 15:11:58.557808: val_loss -0.3256 
2023-05-02 15:12:00.559746: Pseudo dice [0.7151] 
2023-05-02 15:12:03.375883: Epoch time: 143.17 s 
2023-05-02 15:12:10.133641:  
2023-05-02 15:12:11.490733: Epoch 53 
2023-05-02 15:12:14.012998: Current learning rate: 0.00507 
2023-05-02 15:14:33.898410: train_loss -0.9734 
2023-05-02 15:14:35.247613: val_loss -0.3413 
2023-05-02 15:14:36.795379: Pseudo dice [0.7199] 
2023-05-02 15:14:38.125787: Epoch time: 143.77 s 
2023-05-02 15:14:43.900103:  
2023-05-02 15:14:45.544104: Epoch 54 
2023-05-02 15:14:47.094562: Current learning rate: 0.00497 
2023-05-02 15:17:05.815492: train_loss -0.974 
2023-05-02 15:17:07.216975: val_loss -0.2884 
2023-05-02 15:17:09.646606: Pseudo dice [0.7034] 
2023-05-02 15:17:12.108685: Epoch time: 141.92 s 
2023-05-02 15:17:17.313995:  
2023-05-02 15:17:18.705276: Epoch 55 
2023-05-02 15:17:21.107616: Current learning rate: 0.00487 
2023-05-02 15:19:39.003562: train_loss -0.9745 
2023-05-02 15:19:40.354927: val_loss -0.312 
2023-05-02 15:19:42.451106: Pseudo dice [0.7087] 
2023-05-02 15:19:45.265479: Epoch time: 141.69 s 
2023-05-02 15:19:51.304637:  
2023-05-02 15:19:52.941095: Epoch 56 
2023-05-02 15:19:55.700161: Current learning rate: 0.00478 
2023-05-02 15:22:14.668073: train_loss -0.9747 
2023-05-02 15:22:16.174187: val_loss -0.3816 
2023-05-02 15:22:18.636985: Pseudo dice [0.7347] 
2023-05-02 15:22:20.193618: Epoch time: 143.36 s 
2023-05-02 15:22:26.947770:  
2023-05-02 15:22:28.279603: Epoch 57 
2023-05-02 15:22:30.409161: Current learning rate: 0.00468 
2023-05-02 15:24:48.533352: train_loss -0.9751 
2023-05-02 15:24:50.062141: val_loss -0.3769 
2023-05-02 15:24:52.265836: Pseudo dice [0.7363] 
2023-05-02 15:24:54.566576: Epoch time: 141.59 s 
2023-05-02 15:24:56.868959: Yayy! New best EMA pseudo Dice: 0.7161 
2023-05-02 15:25:08.166325:  
2023-05-02 15:25:09.527531: Epoch 58 
2023-05-02 15:25:10.965402: Current learning rate: 0.00458 
2023-05-02 15:27:29.011008: train_loss -0.9748 
2023-05-02 15:27:30.414116: val_loss -0.3386 
2023-05-02 15:27:31.747516: Pseudo dice [0.7237] 
2023-05-02 15:27:34.194489: Epoch time: 140.85 s 
2023-05-02 15:27:36.612502: Yayy! New best EMA pseudo Dice: 0.7169 
2023-05-02 15:27:46.565093:  
2023-05-02 15:27:48.121491: Epoch 59 
2023-05-02 15:27:50.975599: Current learning rate: 0.00448 
2023-05-02 15:30:08.174963: train_loss -0.9755 
2023-05-02 15:30:09.412325: val_loss -0.298 
2023-05-02 15:30:10.768592: Pseudo dice [0.6992] 
2023-05-02 15:30:13.213338: Epoch time: 141.61 s 
2023-05-02 15:30:24.653263:  
2023-05-02 15:30:26.148462: Epoch 60 
2023-05-02 15:30:28.361101: Current learning rate: 0.00438 
2023-05-02 15:32:48.951995: train_loss -0.9751 
2023-05-02 15:32:50.280093: val_loss -0.3179 
2023-05-02 15:32:51.841630: Pseudo dice [0.7106] 
2023-05-02 15:32:53.267999: Epoch time: 144.3 s 
2023-05-02 15:32:59.875552:  
2023-05-02 15:33:01.163735: Epoch 61 
2023-05-02 15:33:04.054513: Current learning rate: 0.00429 
2023-05-02 15:35:21.985284: train_loss -0.9759 
2023-05-02 15:35:23.329057: val_loss -0.3013 
2023-05-02 15:35:26.126111: Pseudo dice [0.6992] 
2023-05-02 15:35:27.573302: Epoch time: 142.11 s 
2023-05-02 15:35:34.382918:  
2023-05-02 15:35:35.744461: Epoch 62 
2023-05-02 15:35:37.312404: Current learning rate: 0.00419 
2023-05-02 15:37:57.171259: train_loss -0.9754 
2023-05-02 15:37:58.760095: val_loss -0.2871 
2023-05-02 15:38:01.070807: Pseudo dice [0.7051] 
2023-05-02 15:38:03.570542: Epoch time: 142.79 s 
2023-05-02 15:38:09.252330:  
2023-05-02 15:38:10.924203: Epoch 63 
2023-05-02 15:38:12.242358: Current learning rate: 0.00409 
2023-05-02 15:40:30.866832: train_loss -0.9763 
2023-05-02 15:40:32.364782: val_loss -0.3201 
2023-05-02 15:40:33.936834: Pseudo dice [0.7126] 
2023-05-02 15:40:35.643774: Epoch time: 141.62 s 
2023-05-02 15:40:41.682375:  
2023-05-02 15:40:43.253008: Epoch 64 
2023-05-02 15:40:44.859761: Current learning rate: 0.00399 
2023-05-02 15:43:04.242906: train_loss -0.9759 
2023-05-02 15:43:05.545727: val_loss -0.3728 
2023-05-02 15:43:07.753259: Pseudo dice [0.7271] 
2023-05-02 15:43:09.691030: Epoch time: 142.56 s 
2023-05-02 15:43:15.538569:  
2023-05-02 15:43:16.924234: Epoch 65 
2023-05-02 15:43:19.654632: Current learning rate: 0.00389 
2023-05-02 15:45:39.086826: train_loss -0.9767 
2023-05-02 15:45:40.498220: val_loss -0.3247 
2023-05-02 15:45:42.909637: Pseudo dice [0.7203] 
2023-05-02 15:45:44.384567: Epoch time: 143.55 s 
2023-05-02 15:45:50.644398:  
2023-05-02 15:45:52.025385: Epoch 66 
2023-05-02 15:45:54.721371: Current learning rate: 0.00379 
2023-05-02 15:48:16.866177: train_loss -0.9766 
2023-05-02 15:48:18.528645: val_loss -0.3352 
2023-05-02 15:48:20.183316: Pseudo dice [0.7182] 
2023-05-02 15:48:22.743490: Epoch time: 146.22 s 
2023-05-02 15:48:29.441754:  
2023-05-02 15:48:30.796184: Epoch 67 
2023-05-02 15:48:33.300806: Current learning rate: 0.00369 
2023-05-02 15:50:50.623060: train_loss -0.9767 
2023-05-02 15:50:52.227051: val_loss -0.3578 
2023-05-02 15:50:54.662652: Pseudo dice [0.7267] 
2023-05-02 15:50:56.234628: Epoch time: 141.18 s 
2023-05-02 15:51:01.713653:  
2023-05-02 15:51:03.232251: Epoch 68 
2023-05-02 15:51:04.630514: Current learning rate: 0.00359 
2023-05-02 15:53:27.082023: train_loss -0.9769 
2023-05-02 15:53:28.796118: val_loss -0.3398 
2023-05-02 15:53:30.276350: Pseudo dice [0.7159] 
2023-05-02 15:53:33.432872: Epoch time: 145.37 s 
2023-05-02 15:53:40.635015:  
2023-05-02 15:53:42.299421: Epoch 69 
2023-05-02 15:53:44.728484: Current learning rate: 0.00349 
2023-05-02 15:56:06.005135: train_loss -0.977 
2023-05-02 15:56:07.661485: val_loss -0.2826 
2023-05-02 15:56:10.236972: Pseudo dice [0.6949] 
2023-05-02 15:56:12.372264: Epoch time: 145.37 s 
2023-05-02 15:56:22.564827:  
2023-05-02 15:56:23.884800: Epoch 70 
2023-05-02 15:56:26.379974: Current learning rate: 0.00338 
2023-05-02 15:58:47.353054: train_loss -0.9768 
2023-05-02 15:58:48.938376: val_loss -0.3426 
2023-05-02 15:58:50.318419: Pseudo dice [0.7263] 
2023-05-02 15:58:53.179783: Epoch time: 144.79 s 
2023-05-02 15:58:58.875977:  
2023-05-02 15:59:00.796082: Epoch 71 
2023-05-02 15:59:04.020622: Current learning rate: 0.00328 
2023-05-02 16:01:22.502168: train_loss -0.977 
2023-05-02 16:01:23.985522: val_loss -0.3461 
2023-05-02 16:01:25.510960: Pseudo dice [0.7217] 
2023-05-02 16:01:27.683643: Epoch time: 143.63 s 
2023-05-02 16:01:33.961405:  
2023-05-02 16:01:35.356540: Epoch 72 
2023-05-02 16:01:37.909828: Current learning rate: 0.00318 
2023-05-02 16:03:55.678849: train_loss -0.9776 
2023-05-02 16:03:57.176296: val_loss -0.3096 
2023-05-02 16:04:00.230181: Pseudo dice [0.7044] 
2023-05-02 16:04:01.600544: Epoch time: 141.72 s 
2023-05-02 16:04:07.682366:  
2023-05-02 16:04:09.401889: Epoch 73 
2023-05-02 16:04:11.896300: Current learning rate: 0.00308 
2023-05-02 16:06:29.619300: train_loss -0.9778 
2023-05-02 16:06:31.182300: val_loss -0.2385 
2023-05-02 16:06:32.984819: Pseudo dice [0.6737] 
2023-05-02 16:06:36.683894: Epoch time: 141.94 s 
2023-05-02 16:06:42.704499:  
2023-05-02 16:06:43.964252: Epoch 74 
2023-05-02 16:06:46.856176: Current learning rate: 0.00297 
2023-05-02 16:09:04.962928: train_loss -0.9781 
2023-05-02 16:09:06.450719: val_loss -0.3248 
2023-05-02 16:09:08.132436: Pseudo dice [0.7097] 
2023-05-02 16:09:10.552875: Epoch time: 142.26 s 
2023-05-02 16:09:15.648004:  
2023-05-02 16:09:17.632791: Epoch 75 
2023-05-02 16:09:20.433324: Current learning rate: 0.00287 
2023-05-02 16:11:39.327533: train_loss -0.9779 
2023-05-02 16:11:40.833299: val_loss -0.3186 
2023-05-02 16:11:43.625549: Pseudo dice [0.7204] 
2023-05-02 16:11:45.141402: Epoch time: 143.68 s 
2023-05-02 16:11:51.754751:  
2023-05-02 16:11:53.104033: Epoch 76 
2023-05-02 16:11:55.694134: Current learning rate: 0.00277 
2023-05-02 16:14:17.178262: train_loss -0.9785 
2023-05-02 16:14:18.558043: val_loss -0.2939 
2023-05-02 16:14:21.110718: Pseudo dice [0.7024] 
2023-05-02 16:14:22.582149: Epoch time: 145.42 s 
2023-05-02 16:14:28.617568:  
2023-05-02 16:14:30.451166: Epoch 77 
2023-05-02 16:14:32.932360: Current learning rate: 0.00266 
2023-05-02 16:16:51.258846: train_loss -0.9788 
2023-05-02 16:16:52.621386: val_loss -0.3487 
2023-05-02 16:16:55.340032: Pseudo dice [0.7207] 
2023-05-02 16:16:57.862373: Epoch time: 142.64 s 
2023-05-02 16:17:04.617335:  
2023-05-02 16:17:06.459285: Epoch 78 
2023-05-02 16:17:09.849498: Current learning rate: 0.00256 
2023-05-02 16:19:30.949129: train_loss -0.9789 
2023-05-02 16:19:32.380551: val_loss -0.3679 
2023-05-02 16:19:34.711709: Pseudo dice [0.7331] 
2023-05-02 16:19:37.279287: Epoch time: 146.33 s 
2023-05-02 16:19:43.258417:  
2023-05-02 16:19:44.781967: Epoch 79 
2023-05-02 16:19:46.281968: Current learning rate: 0.00245 
2023-05-02 16:22:03.586826: train_loss -0.9793 
2023-05-02 16:22:05.194350: val_loss -0.3458 
2023-05-02 16:22:06.601506: Pseudo dice [0.7174] 
2023-05-02 16:22:08.356976: Epoch time: 140.33 s 
2023-05-02 16:22:17.906930:  
2023-05-02 16:22:19.149035: Epoch 80 
2023-05-02 16:22:21.346759: Current learning rate: 0.00235 
2023-05-02 16:24:40.319905: train_loss -0.9786 
2023-05-02 16:24:41.814461: val_loss -0.3669 
2023-05-02 16:24:43.345859: Pseudo dice [0.7252] 
2023-05-02 16:24:44.898162: Epoch time: 142.41 s 
2023-05-02 16:24:50.483398:  
2023-05-02 16:24:52.075741: Epoch 81 
2023-05-02 16:24:54.480046: Current learning rate: 0.00224 
2023-05-02 16:27:11.771665: train_loss -0.9791 
2023-05-02 16:27:13.089983: val_loss -0.3589 
2023-05-02 16:27:15.364816: Pseudo dice [0.728] 
2023-05-02 16:27:17.574953: Epoch time: 141.29 s 
2023-05-02 16:27:24.554804:  
2023-05-02 16:27:25.893686: Epoch 82 
2023-05-02 16:27:28.887080: Current learning rate: 0.00214 
2023-05-02 16:29:46.302751: train_loss -0.9794 
2023-05-02 16:29:47.795846: val_loss -0.3413 
2023-05-02 16:29:50.093538: Pseudo dice [0.7205] 
2023-05-02 16:29:52.413489: Epoch time: 141.75 s 
2023-05-02 16:29:59.208791:  
2023-05-02 16:30:00.880341: Epoch 83 
2023-05-02 16:30:02.977039: Current learning rate: 0.00203 
2023-05-02 16:32:22.265133: train_loss -0.9789 
2023-05-02 16:32:24.457926: val_loss -0.2926 
2023-05-02 16:32:27.501096: Pseudo dice [0.7045] 
2023-05-02 16:32:29.953652: Epoch time: 143.06 s 
2023-05-02 16:32:35.891416:  
2023-05-02 16:32:37.246041: Epoch 84 
2023-05-02 16:32:39.712080: Current learning rate: 0.00192 
2023-05-02 16:34:58.741736: train_loss -0.9791 
2023-05-02 16:35:00.207578: val_loss -0.3732 
2023-05-02 16:35:02.471880: Pseudo dice [0.7378] 
2023-05-02 16:35:04.749140: Epoch time: 142.85 s 
2023-05-02 16:35:07.521139: Yayy! New best EMA pseudo Dice: 0.7179 
2023-05-02 16:35:16.678630:  
2023-05-02 16:35:17.974814: Epoch 85 
2023-05-02 16:35:20.488240: Current learning rate: 0.00181 
2023-05-02 16:37:37.620245: train_loss -0.9796 
2023-05-02 16:37:39.201351: val_loss -0.2813 
2023-05-02 16:37:42.253670: Pseudo dice [0.6904] 
2023-05-02 16:37:44.798362: Epoch time: 140.94 s 
2023-05-02 16:37:51.882363:  
2023-05-02 16:37:53.256475: Epoch 86 
2023-05-02 16:37:54.891324: Current learning rate: 0.0017 
2023-05-02 16:40:14.265749: train_loss -0.9794 
2023-05-02 16:40:15.607121: val_loss -0.317 
2023-05-02 16:40:16.970996: Pseudo dice [0.7099] 
2023-05-02 16:40:19.312220: Epoch time: 142.38 s 
2023-05-02 16:40:24.449266:  
2023-05-02 16:40:25.942343: Epoch 87 
2023-05-02 16:40:27.967169: Current learning rate: 0.00159 
2023-05-02 16:42:46.404424: train_loss -0.9793 
2023-05-02 16:42:48.046698: val_loss -0.3422 
2023-05-02 16:42:49.461357: Pseudo dice [0.7174] 
2023-05-02 16:42:50.993141: Epoch time: 141.96 s 
2023-05-02 16:42:57.140236:  
2023-05-02 16:42:58.628340: Epoch 88 
2023-05-02 16:43:01.086975: Current learning rate: 0.00148 
2023-05-02 16:45:21.285638: train_loss -0.9798 
2023-05-02 16:45:22.957852: val_loss -0.3076 
2023-05-02 16:45:25.915895: Pseudo dice [0.7111] 
2023-05-02 16:45:28.040780: Epoch time: 144.15 s 
2023-05-02 16:45:34.293665:  
2023-05-02 16:45:35.816258: Epoch 89 
2023-05-02 16:45:38.188686: Current learning rate: 0.00137 
2023-05-02 16:47:58.629027: train_loss -0.9798 
2023-05-02 16:48:00.146894: val_loss -0.3073 
2023-05-02 16:48:02.755432: Pseudo dice [0.7085] 
2023-05-02 16:48:05.095586: Epoch time: 144.34 s 
2023-05-02 16:48:15.948848:  
2023-05-02 16:48:17.479143: Epoch 90 
2023-05-02 16:48:20.282741: Current learning rate: 0.00126 
2023-05-02 16:50:39.139136: train_loss -0.9801 
2023-05-02 16:50:40.773310: val_loss -0.3723 
2023-05-02 16:50:42.418899: Pseudo dice [0.738] 
2023-05-02 16:50:44.709988: Epoch time: 143.19 s 
2023-05-02 16:50:51.569374:  
2023-05-02 16:50:53.416845: Epoch 91 
2023-05-02 16:50:54.827303: Current learning rate: 0.00115 
2023-05-02 16:53:13.382293: train_loss -0.9797 
2023-05-02 16:53:14.970265: val_loss -0.3357 
2023-05-02 16:53:16.462316: Pseudo dice [0.7202] 
2023-05-02 16:53:18.819850: Epoch time: 141.81 s 
2023-05-02 16:53:25.206077:  
2023-05-02 16:53:26.900069: Epoch 92 
2023-05-02 16:53:28.226790: Current learning rate: 0.00103 
2023-05-02 16:55:48.690646: train_loss -0.9799 
2023-05-02 16:55:50.480353: val_loss -0.3165 
2023-05-02 16:55:52.035162: Pseudo dice [0.7013] 
2023-05-02 16:55:54.410936: Epoch time: 143.49 s 
2023-05-02 16:56:00.049358:  
2023-05-02 16:56:01.447329: Epoch 93 
2023-05-02 16:56:03.995837: Current learning rate: 0.00091 
2023-05-02 16:58:22.488399: train_loss -0.9801 
2023-05-02 16:58:24.077286: val_loss -0.3173 
2023-05-02 16:58:25.734811: Pseudo dice [0.7161] 
2023-05-02 16:58:28.137393: Epoch time: 142.44 s 
2023-05-02 16:58:33.436966:  
2023-05-02 16:58:35.623874: Epoch 94 
2023-05-02 16:58:37.690217: Current learning rate: 0.00079 
2023-05-02 17:00:59.180545: train_loss -0.9801 
2023-05-02 17:01:00.613712: val_loss -0.281 
2023-05-02 17:01:02.732324: Pseudo dice [0.6907] 
2023-05-02 17:01:05.024809: Epoch time: 145.74 s 
2023-05-02 17:01:10.864728:  
2023-05-02 17:01:12.262650: Epoch 95 
2023-05-02 17:01:14.355594: Current learning rate: 0.00067 
2023-05-02 17:03:35.119859: train_loss -0.9799 
2023-05-02 17:03:37.176890: val_loss -0.3 
2023-05-02 17:03:39.410649: Pseudo dice [0.7036] 
2023-05-02 17:03:40.831107: Epoch time: 144.26 s 
2023-05-02 17:03:47.492287:  
2023-05-02 17:03:49.145546: Epoch 96 
2023-05-02 17:03:51.668356: Current learning rate: 0.00055 
2023-05-02 17:06:12.304400: train_loss -0.9803 
2023-05-02 17:06:13.996343: val_loss -0.3722 
2023-05-02 17:06:16.953109: Pseudo dice [0.7389] 
2023-05-02 17:06:18.444071: Epoch time: 144.81 s 
2023-05-02 17:06:24.737081:  
2023-05-02 17:06:26.144271: Epoch 97 
2023-05-02 17:06:28.709572: Current learning rate: 0.00043 
2023-05-02 17:08:51.711407: train_loss -0.9799 
2023-05-02 17:08:53.148951: val_loss -0.3107 
2023-05-02 17:08:54.569051: Pseudo dice [0.7076] 
2023-05-02 17:08:56.799653: Epoch time: 146.98 s 
2023-05-02 17:09:03.838775:  
2023-05-02 17:09:05.272168: Epoch 98 
2023-05-02 17:09:07.766117: Current learning rate: 0.0003 
2023-05-02 17:11:30.864223: train_loss -0.9806 
2023-05-02 17:11:32.282541: val_loss -0.3306 
2023-05-02 17:11:34.866703: Pseudo dice [0.7138] 
2023-05-02 17:11:36.213016: Epoch time: 147.03 s 
2023-05-02 17:11:43.658341:  
2023-05-02 17:11:45.283822: Epoch 99 
2023-05-02 17:11:47.413521: Current learning rate: 0.00016 
2023-05-02 17:14:04.812790: train_loss -0.9806 
2023-05-02 17:14:06.267248: val_loss -0.3104 
2023-05-02 17:14:07.837793: Pseudo dice [0.7111] 
2023-05-02 17:14:09.443060: Epoch time: 141.16 s 
2023-05-02 17:14:20.527780: Training done. 
2023-05-02 17:14:23.567142: Using splits from existing split file: drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_Liver/splits_final.json 
2023-05-02 17:14:25.834599: The split file contains 5 splits. 
2023-05-02 17:14:28.249054: Desired fold for training: 4 
2023-05-02 17:14:29.677156: This split has 13 training and 3 validation cases. 
2023-05-02 17:14:32.008594: predicting 15 
2023-05-02 17:14:35.138815: predicting 21 
2023-05-02 17:14:38.015993: predicting 38 
2023-05-02 17:15:50.423223: Validation complete 
2023-05-02 17:15:52.026147: Mean Validation Dice:  0.6407864156437592 
