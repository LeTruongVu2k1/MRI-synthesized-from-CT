
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 23, 'patch_size': [256, 192], 'median_image_size_in_voxels': [251.5, 186.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Liver', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [30, 252, 186], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.2606104910373688, 'median': 0.24769647419452667, 'min': 0.0, 'percentile_00_5': 0.06384892016649246, 'percentile_99_5': 0.6531020402908325, 'std': 0.0987810268998146}}} 
 
2023-05-02 15:28:01.801617: unpacking dataset... 
2023-05-02 15:29:43.060594: unpacking done... 
2023-05-02 15:29:44.124998: do_dummy_2d_data_aug: False 
2023-05-02 15:29:46.193679: Using splits from existing split file: drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_Liver/splits_final.json 
2023-05-02 15:29:47.814531: The split file contains 5 splits. 
2023-05-02 15:29:49.068054: Desired fold for training: 2 
2023-05-02 15:29:51.272469: This split has 13 training and 3 validation cases. 
2023-05-02 15:30:15.072396: Unable to plot network architecture: 
2023-05-02 15:30:16.458321: No module named 'hiddenlayer' 
2023-05-02 15:30:22.073240:  
2023-05-02 15:30:23.142174: Epoch 24 
2023-05-02 15:30:24.389253: Current learning rate: 0.00781 
2023-05-02 15:34:25.062192: train_loss -0.9621 
2023-05-02 15:34:26.323979: val_loss -0.821 
2023-05-02 15:34:27.491228: Pseudo dice [0.8999] 
2023-05-02 15:34:29.604319: Epoch time: 242.99 s 
2023-05-02 15:34:31.625688: Yayy! New best EMA pseudo Dice: 0.907 
2023-05-02 15:34:43.449814:  
2023-05-02 15:34:44.741957: Epoch 25 
2023-05-02 15:34:45.942355: Current learning rate: 0.00772 
2023-05-02 15:37:11.056710: train_loss -0.9606 
2023-05-02 15:37:12.226230: val_loss -0.831 
2023-05-02 15:37:13.315211: Pseudo dice [0.9053] 
2023-05-02 15:37:14.617968: Epoch time: 147.61 s 
2023-05-02 15:37:19.488105:  
2023-05-02 15:37:20.766399: Epoch 26 
2023-05-02 15:37:22.674427: Current learning rate: 0.00763 
2023-05-02 15:39:49.419030: train_loss -0.9615 
2023-05-02 15:39:50.602348: val_loss -0.8522 
2023-05-02 15:39:51.821395: Pseudo dice [0.9186] 
2023-05-02 15:39:53.988457: Epoch time: 149.93 s 
2023-05-02 15:39:55.672326: Yayy! New best EMA pseudo Dice: 0.908 
2023-05-02 15:40:05.295549:  
2023-05-02 15:40:06.343016: Epoch 27 
2023-05-02 15:40:07.649395: Current learning rate: 0.00753 
2023-05-02 15:42:34.868901: train_loss -0.9629 
2023-05-02 15:42:36.041860: val_loss -0.8591 
2023-05-02 15:42:38.176264: Pseudo dice [0.9219] 
2023-05-02 15:42:39.335118: Epoch time: 149.57 s 
2023-05-02 15:42:41.353015: Yayy! New best EMA pseudo Dice: 0.9094 
2023-05-02 15:42:50.253910:  
2023-05-02 15:42:51.470426: Epoch 28 
2023-05-02 15:42:53.509079: Current learning rate: 0.00744 
2023-05-02 15:45:18.414904: train_loss -0.9629 
2023-05-02 15:45:19.599551: val_loss -0.8351 
2023-05-02 15:45:20.782061: Pseudo dice [0.9075] 
2023-05-02 15:45:22.901212: Epoch time: 148.16 s 
2023-05-02 15:45:28.205412:  
2023-05-02 15:45:29.340979: Epoch 29 
2023-05-02 15:45:30.495146: Current learning rate: 0.00735 
2023-05-02 15:47:57.247507: train_loss -0.9637 
2023-05-02 15:47:58.582278: val_loss -0.8549 
2023-05-02 15:48:00.076896: Pseudo dice [0.9197] 
2023-05-02 15:48:01.343545: Epoch time: 149.04 s 
2023-05-02 15:48:06.109002: Yayy! New best EMA pseudo Dice: 0.9103 
2023-05-02 15:48:15.379888:  
2023-05-02 15:48:16.561946: Epoch 30 
2023-05-02 15:48:18.785440: Current learning rate: 0.00725 
2023-05-02 15:50:40.894123: train_loss -0.9661 
2023-05-02 15:50:42.304562: val_loss -0.8564 
2023-05-02 15:50:46.690685: Pseudo dice [0.9201] 
2023-05-02 15:50:48.818069: Epoch time: 145.52 s 
2023-05-02 15:50:51.291621: Yayy! New best EMA pseudo Dice: 0.9112 
2023-05-02 15:51:00.500374:  
2023-05-02 15:51:01.810915: Epoch 31 
2023-05-02 15:51:02.910573: Current learning rate: 0.00716 
2023-05-02 15:53:29.612466: train_loss -0.9662 
2023-05-02 15:53:30.932313: val_loss -0.8705 
2023-05-02 15:53:33.159158: Pseudo dice [0.9282] 
2023-05-02 15:53:34.685524: Epoch time: 149.11 s 
2023-05-02 15:53:37.893282: Yayy! New best EMA pseudo Dice: 0.9129 
2023-05-02 15:53:46.783209:  
2023-05-02 15:53:47.987706: Epoch 32 
2023-05-02 15:53:49.217829: Current learning rate: 0.00707 
2023-05-02 15:56:14.094467: train_loss -0.9667 
2023-05-02 15:56:15.548609: val_loss -0.8257 
2023-05-02 15:56:15.558690: Pseudo dice [0.9062] 
2023-05-02 15:56:16.984951: Epoch time: 147.31 s 
2023-05-02 15:56:22.931387:  
2023-05-02 15:56:24.401265: Epoch 33 
2023-05-02 15:56:25.505429: Current learning rate: 0.00697 
2023-05-02 15:58:51.852004: train_loss -0.9668 
2023-05-02 15:58:53.310573: val_loss -0.8744 
2023-05-02 15:58:54.669881: Pseudo dice [0.9307] 
2023-05-02 15:58:57.148216: Epoch time: 148.92 s 
2023-05-02 15:58:59.225294: Yayy! New best EMA pseudo Dice: 0.9141 
2023-05-02 15:59:11.974160:  
2023-05-02 15:59:13.330803: Epoch 34 
2023-05-02 15:59:15.584872: Current learning rate: 0.00688 
2023-05-02 16:01:38.191421: train_loss -0.9679 
2023-05-02 16:01:39.673515: val_loss -0.8672 
2023-05-02 16:01:42.317672: Pseudo dice [0.9253] 
2023-05-02 16:01:43.854989: Epoch time: 146.22 s 
2023-05-02 16:01:45.847988: Yayy! New best EMA pseudo Dice: 0.9152 
2023-05-02 16:01:55.162510:  
2023-05-02 16:01:56.527136: Epoch 35 
2023-05-02 16:01:57.949680: Current learning rate: 0.00679 
2023-05-02 16:04:23.401196: train_loss -0.9685 
2023-05-02 16:04:24.664641: val_loss -0.8469 
2023-05-02 16:04:26.091364: Pseudo dice [0.9173] 
2023-05-02 16:04:31.830538: Epoch time: 148.24 s 
2023-05-02 16:04:34.028951: Yayy! New best EMA pseudo Dice: 0.9154 
2023-05-02 16:04:43.748268:  
2023-05-02 16:04:45.088759: Epoch 36 
2023-05-02 16:04:47.409364: Current learning rate: 0.00669 
2023-05-02 16:07:12.076587: train_loss -0.9695 
2023-05-02 16:07:13.318960: val_loss -0.8421 
2023-05-02 16:07:14.557399: Pseudo dice [0.9124] 
2023-05-02 16:07:16.593511: Epoch time: 148.33 s 
2023-05-02 16:07:24.564129:  
2023-05-02 16:07:25.648169: Epoch 37 
2023-05-02 16:07:27.768852: Current learning rate: 0.0066 
2023-05-02 16:09:55.439072: train_loss -0.9694 
2023-05-02 16:09:56.767668: val_loss -0.8611 
2023-05-02 16:09:58.179145: Pseudo dice [0.9242] 
2023-05-02 16:09:59.451309: Epoch time: 150.88 s 
2023-05-02 16:10:01.778637: Yayy! New best EMA pseudo Dice: 0.916 
2023-05-02 16:10:11.168548:  
2023-05-02 16:10:12.494035: Epoch 38 
2023-05-02 16:10:14.471823: Current learning rate: 0.0065 
2023-05-02 16:12:39.249029: train_loss -0.9704 
2023-05-02 16:12:40.630684: val_loss -0.8628 
2023-05-02 16:12:41.803016: Pseudo dice [0.9259] 
2023-05-02 16:12:43.207934: Epoch time: 148.08 s 
2023-05-02 16:12:45.479737: Yayy! New best EMA pseudo Dice: 0.917 
2023-05-02 16:12:56.333147:  
2023-05-02 16:12:57.466076: Epoch 39 
2023-05-02 16:12:59.620355: Current learning rate: 0.00641 
2023-05-02 16:15:26.710111: train_loss -0.9699 
2023-05-02 16:15:28.078892: val_loss -0.8349 
2023-05-02 16:15:29.364014: Pseudo dice [0.9103] 
2023-05-02 16:15:30.726464: Epoch time: 150.38 s 
2023-05-02 16:15:40.109192:  
2023-05-02 16:15:41.360738: Epoch 40 
2023-05-02 16:15:42.636974: Current learning rate: 0.00631 
2023-05-02 16:18:10.485603: train_loss -0.9684 
2023-05-02 16:18:11.704010: val_loss -0.8464 
2023-05-02 16:18:13.795136: Pseudo dice [0.9148] 
2023-05-02 16:18:15.718431: Epoch time: 150.38 s 
2023-05-02 16:18:21.094689:  
2023-05-02 16:18:22.313107: Epoch 41 
2023-05-02 16:18:23.662450: Current learning rate: 0.00622 
2023-05-02 16:20:48.780312: train_loss -0.9709 
2023-05-02 16:20:49.996729: val_loss -0.8637 
2023-05-02 16:20:51.435358: Pseudo dice [0.9235] 
2023-05-02 16:20:53.818405: Epoch time: 147.69 s 
2023-05-02 16:21:00.992397:  
2023-05-02 16:21:02.133199: Epoch 42 
2023-05-02 16:21:03.369004: Current learning rate: 0.00612 
2023-05-02 16:23:31.567314: train_loss -0.9711 
2023-05-02 16:23:32.774249: val_loss -0.8737 
2023-05-02 16:23:35.226732: Pseudo dice [0.9318] 
2023-05-02 16:23:36.592865: Epoch time: 150.58 s 
2023-05-02 16:23:40.047258: Yayy! New best EMA pseudo Dice: 0.9184 
2023-05-02 16:23:51.154975:  
2023-05-02 16:23:52.490198: Epoch 43 
2023-05-02 16:23:54.805112: Current learning rate: 0.00603 
2023-05-02 16:26:23.356392: train_loss -0.9703 
2023-05-02 16:26:24.690770: val_loss -0.7973 
2023-05-02 16:26:27.006969: Pseudo dice [0.8873] 
2023-05-02 16:26:38.400774: Epoch time: 152.2 s 
2023-05-02 16:26:44.624674:  
2023-05-02 16:26:46.071545: Epoch 44 
2023-05-02 16:26:50.050727: Current learning rate: 0.00593 
2023-05-02 16:29:17.596281: train_loss -0.9713 
2023-05-02 16:29:18.932188: val_loss -0.8438 
2023-05-02 16:29:21.342603: Pseudo dice [0.9124] 
2023-05-02 16:29:23.632287: Epoch time: 152.97 s 
2023-05-02 16:29:28.762237:  
2023-05-02 16:29:30.066719: Epoch 45 
2023-05-02 16:29:34.016019: Current learning rate: 0.00584 
2023-05-02 16:31:59.969115: train_loss -0.9727 
2023-05-02 16:32:01.353510: val_loss -0.8626 
2023-05-02 16:32:03.671632: Pseudo dice [0.926] 
2023-05-02 16:32:04.909813: Epoch time: 151.21 s 
2023-05-02 16:32:11.032307:  
2023-05-02 16:32:12.305724: Epoch 46 
2023-05-02 16:32:13.645418: Current learning rate: 0.00574 
2023-05-02 16:34:41.151365: train_loss -0.9733 
2023-05-02 16:34:42.455729: val_loss -0.8351 
2023-05-02 16:34:44.445702: Pseudo dice [0.9076] 
2023-05-02 16:34:45.672836: Epoch time: 150.12 s 
2023-05-02 16:34:51.879465:  
2023-05-02 16:34:53.071242: Epoch 47 
2023-05-02 16:34:55.181380: Current learning rate: 0.00565 
2023-05-02 16:37:22.506253: train_loss -0.9718 
2023-05-02 16:37:23.723364: val_loss -0.8548 
2023-05-02 16:37:25.586537: Pseudo dice [0.9201] 
2023-05-02 16:37:28.037764: Epoch time: 150.63 s 
2023-05-02 16:37:33.572695:  
2023-05-02 16:37:35.028591: Epoch 48 
2023-05-02 16:37:36.441931: Current learning rate: 0.00555 
2023-05-02 16:40:00.766919: train_loss -0.9734 
2023-05-02 16:40:02.016540: val_loss -0.8356 
2023-05-02 16:40:03.241038: Pseudo dice [0.9113] 
2023-05-02 16:40:04.571699: Epoch time: 147.2 s 
2023-05-02 16:40:10.032316:  
2023-05-02 16:40:11.364912: Epoch 49 
2023-05-02 16:40:12.518188: Current learning rate: 0.00546 
2023-05-02 16:42:35.627913: train_loss -0.9738 
2023-05-02 16:42:36.886429: val_loss -0.8499 
2023-05-02 16:42:39.092230: Pseudo dice [0.9188] 
2023-05-02 16:42:41.399426: Epoch time: 145.6 s 
2023-05-02 16:42:50.822652:  
2023-05-02 16:42:52.042428: Epoch 50 
2023-05-02 16:42:54.371370: Current learning rate: 0.00536 
2023-05-02 16:45:18.544308: train_loss -0.9728 
2023-05-02 16:45:19.850087: val_loss -0.8653 
2023-05-02 16:45:22.090313: Pseudo dice [0.926] 
2023-05-02 16:45:23.290605: Epoch time: 147.72 s 
2023-05-02 16:45:29.266926:  
2023-05-02 16:45:30.682432: Epoch 51 
2023-05-02 16:45:31.829535: Current learning rate: 0.00526 
2023-05-02 16:47:55.647476: train_loss -0.9746 
2023-05-02 16:47:56.903310: val_loss -0.8616 
2023-05-02 16:47:59.170204: Pseudo dice [0.9246] 
2023-05-02 16:48:01.578256: Epoch time: 146.38 s 
2023-05-02 16:48:07.203142:  
2023-05-02 16:48:09.452985: Epoch 52 
2023-05-02 16:48:11.660342: Current learning rate: 0.00517 
2023-05-02 16:50:37.655607: train_loss -0.9743 
2023-05-02 16:50:38.980198: val_loss -0.8581 
2023-05-02 16:50:40.284156: Pseudo dice [0.9216] 
2023-05-02 16:50:42.817614: Epoch time: 150.45 s 
2023-05-02 16:50:48.861401:  
2023-05-02 16:50:50.085283: Epoch 53 
2023-05-02 16:50:51.283626: Current learning rate: 0.00507 
2023-05-02 16:53:18.426826: train_loss -0.9746 
2023-05-02 16:53:19.655890: val_loss -0.863 
2023-05-02 16:53:21.163433: Pseudo dice [0.9255] 
2023-05-02 16:53:23.651766: Epoch time: 149.57 s 
2023-05-02 16:53:24.939441: Yayy! New best EMA pseudo Dice: 0.9187 
2023-05-02 16:53:34.066400:  
2023-05-02 16:53:35.470726: Epoch 54 
2023-05-02 16:53:37.440105: Current learning rate: 0.00497 
2023-05-02 16:56:09.781704: train_loss -0.9752 
2023-05-02 16:56:11.413999: val_loss -0.8618 
2023-05-02 16:56:13.534973: Pseudo dice [0.9259] 
2023-05-02 16:56:15.709669: Epoch time: 155.72 s 
2023-05-02 16:56:16.943042: Yayy! New best EMA pseudo Dice: 0.9194 
2023-05-02 16:56:27.459764:  
2023-05-02 16:56:28.911870: Epoch 55 
2023-05-02 16:56:30.030508: Current learning rate: 0.00487 
2023-05-02 16:58:54.779712: train_loss -0.9758 
2023-05-02 16:58:56.053176: val_loss -0.868 
2023-05-02 16:58:57.388366: Pseudo dice [0.929] 
2023-05-02 16:58:59.667660: Epoch time: 147.32 s 
2023-05-02 16:59:02.386404: Yayy! New best EMA pseudo Dice: 0.9203 
2023-05-02 16:59:12.053989:  
2023-05-02 16:59:13.325534: Epoch 56 
2023-05-02 16:59:14.519465: Current learning rate: 0.00478 
2023-05-02 17:01:42.872886: train_loss -0.9759 
2023-05-02 17:01:44.353001: val_loss -0.8638 
2023-05-02 17:01:46.315425: Pseudo dice [0.9268] 
2023-05-02 17:01:47.628634: Epoch time: 150.82 s 
2023-05-02 17:01:48.883363: Yayy! New best EMA pseudo Dice: 0.921 
2023-05-02 17:01:59.382615:  
2023-05-02 17:02:00.620721: Epoch 57 
2023-05-02 17:02:02.642865: Current learning rate: 0.00468 
2023-05-02 17:04:29.933904: train_loss -0.9756 
2023-05-02 17:04:31.352878: val_loss -0.8703 
2023-05-02 17:04:32.728007: Pseudo dice [0.9275] 
2023-05-02 17:04:34.801479: Epoch time: 150.55 s 
2023-05-02 17:04:37.470079: Yayy! New best EMA pseudo Dice: 0.9216 
2023-05-02 17:04:49.812049:  
2023-05-02 17:04:51.432755: Epoch 58 
2023-05-02 17:04:52.707378: Current learning rate: 0.00458 
2023-05-02 17:07:20.711553: train_loss -0.9749 
2023-05-02 17:07:22.142193: val_loss -0.8261 
2023-05-02 17:07:23.538021: Pseudo dice [0.907] 
2023-05-02 17:07:24.931217: Epoch time: 150.9 s 
2023-05-02 17:07:31.235057:  
2023-05-02 17:07:32.521504: Epoch 59 
2023-05-02 17:07:33.861904: Current learning rate: 0.00448 
2023-05-02 17:09:58.274158: train_loss -0.9764 
2023-05-02 17:09:59.625058: val_loss -0.8574 
2023-05-02 17:10:00.985874: Pseudo dice [0.9237] 
2023-05-02 17:10:03.393870: Epoch time: 147.04 s 
2023-05-02 17:10:13.025259:  
2023-05-02 17:10:14.403334: Epoch 60 
2023-05-02 17:10:16.812705: Current learning rate: 0.00438 
2023-05-02 17:12:43.633053: train_loss -0.9761 
2023-05-02 17:12:44.849625: val_loss -0.8529 
2023-05-02 17:12:46.065722: Pseudo dice [0.9216] 
2023-05-02 17:12:47.971193: Epoch time: 150.61 s 
2023-05-02 17:12:54.068684:  
2023-05-02 17:12:55.297904: Epoch 61 
2023-05-02 17:12:57.264782: Current learning rate: 0.00429 
2023-05-02 17:15:25.735150: train_loss -0.9768 
2023-05-02 17:15:26.975549: val_loss -0.8546 
2023-05-02 17:15:28.319527: Pseudo dice [0.9239] 
2023-05-02 17:15:30.862028: Epoch time: 151.67 s 
2023-05-02 17:15:36.045156:  
2023-05-02 17:15:37.434520: Epoch 62 
2023-05-02 17:15:39.799839: Current learning rate: 0.00419 
2023-05-02 17:18:04.286471: train_loss -0.9772 
2023-05-02 17:18:05.652038: val_loss -0.862 
2023-05-02 17:18:07.629257: Pseudo dice [0.9254] 
2023-05-02 17:18:10.134361: Epoch time: 148.24 s 
2023-05-02 17:18:15.475296:  
2023-05-02 17:18:16.656702: Epoch 63 
2023-05-02 17:18:18.927185: Current learning rate: 0.00409 
2023-05-02 17:20:41.671067: train_loss -0.9775 
2023-05-02 17:20:42.886848: val_loss -0.8599 
2023-05-02 17:20:44.008210: Pseudo dice [0.924] 
2023-05-02 17:20:46.082956: Epoch time: 146.2 s 
2023-05-02 17:20:47.323384: Yayy! New best EMA pseudo Dice: 0.9217 
2023-05-02 17:20:55.636813:  
2023-05-02 17:20:56.819806: Epoch 64 
2023-05-02 17:20:57.900326: Current learning rate: 0.00399 
2023-05-02 17:23:20.363347: train_loss -0.9775 
2023-05-02 17:23:21.695734: val_loss -0.8605 
2023-05-02 17:23:23.905383: Pseudo dice [0.924] 
2023-05-02 17:23:26.116527: Epoch time: 144.73 s 
2023-05-02 17:23:28.048878: Yayy! New best EMA pseudo Dice: 0.9219 
2023-05-02 17:23:36.981697:  
2023-05-02 17:23:38.099425: Epoch 65 
2023-05-02 17:23:40.189973: Current learning rate: 0.00389 
2023-05-02 17:26:03.265312: train_loss -0.9774 
2023-05-02 17:26:04.604034: val_loss -0.8619 
2023-05-02 17:26:06.914522: Pseudo dice [0.9264] 
2023-05-02 17:26:08.206103: Epoch time: 146.28 s 
2023-05-02 17:26:10.537472: Yayy! New best EMA pseudo Dice: 0.9223 
2023-05-02 17:26:21.101898:  
2023-05-02 17:26:22.404303: Epoch 66 
2023-05-02 17:26:24.706470: Current learning rate: 0.00379 
2023-05-02 17:28:49.230356: train_loss -0.9776 
2023-05-02 17:28:50.771132: val_loss -0.8391 
2023-05-02 17:28:53.267392: Pseudo dice [0.915] 
2023-05-02 17:28:54.500723: Epoch time: 148.13 s 
2023-05-02 17:29:00.340394:  
2023-05-02 17:29:01.739213: Epoch 67 
2023-05-02 17:29:03.022637: Current learning rate: 0.00369 
2023-05-02 17:31:29.984693: train_loss -0.9773 
2023-05-02 17:31:31.639652: val_loss -0.8646 
2023-05-02 17:31:33.779127: Pseudo dice [0.9273] 
2023-05-02 17:31:35.042191: Epoch time: 149.65 s 
2023-05-02 17:31:40.904430:  
2023-05-02 17:31:42.161216: Epoch 68 
2023-05-02 17:31:43.440925: Current learning rate: 0.00359 
2023-05-02 17:34:07.532713: train_loss -0.9781 
2023-05-02 17:34:08.694232: val_loss -0.844 
2023-05-02 17:34:09.841894: Pseudo dice [0.9173] 
2023-05-02 17:34:12.055417: Epoch time: 146.63 s 
2023-05-02 17:34:18.562076:  
2023-05-02 17:34:19.816068: Epoch 69 
2023-05-02 17:34:21.762941: Current learning rate: 0.00349 
2023-05-02 17:36:49.167411: train_loss -0.9783 
2023-05-02 17:36:50.450281: val_loss -0.8374 
2023-05-02 17:36:51.846657: Pseudo dice [0.9138] 
2023-05-02 17:36:53.691114: Epoch time: 150.61 s 
2023-05-02 17:37:03.331363:  
2023-05-02 17:37:04.573199: Epoch 70 
2023-05-02 17:37:05.789756: Current learning rate: 0.00338 
2023-05-02 17:39:29.408981: train_loss -0.978 
2023-05-02 17:39:30.609571: val_loss -0.8483 
2023-05-02 17:39:32.723610: Pseudo dice [0.9194] 
2023-05-02 17:39:34.065374: Epoch time: 146.08 s 
2023-05-02 17:39:40.023767:  
2023-05-02 17:39:41.833120: Epoch 71 
2023-05-02 17:39:42.990616: Current learning rate: 0.00328 
2023-05-02 17:42:06.875048: train_loss -0.9782 
2023-05-02 17:42:08.092545: val_loss -0.8504 
2023-05-02 17:42:09.849693: Pseudo dice [0.9215] 
2023-05-02 17:42:11.099061: Epoch time: 146.85 s 
2023-05-02 17:42:18.018633:  
2023-05-02 17:42:19.388905: Epoch 72 
2023-05-02 17:42:21.434761: Current learning rate: 0.00318 
2023-05-02 17:44:46.811437: train_loss -0.9786 
2023-05-02 17:44:48.092489: val_loss -0.8504 
2023-05-02 17:44:51.310342: Pseudo dice [0.9206] 
2023-05-02 17:44:53.413525: Epoch time: 148.79 s 
2023-05-02 17:44:58.441880:  
2023-05-02 17:44:59.798615: Epoch 73 
2023-05-02 17:45:01.094002: Current learning rate: 0.00308 
2023-05-02 17:47:25.612322: train_loss -0.9788 
2023-05-02 17:47:26.701172: val_loss -0.8356 
2023-05-02 17:47:27.893651: Pseudo dice [0.9138] 
2023-05-02 17:47:29.223305: Epoch time: 147.17 s 
2023-05-02 17:47:34.167534:  
2023-05-02 17:47:35.317703: Epoch 74 
2023-05-02 17:47:37.460609: Current learning rate: 0.00297 
2023-05-02 17:50:02.838166: train_loss -0.9788 
2023-05-02 17:50:04.159030: val_loss -0.8605 
2023-05-02 17:50:06.481685: Pseudo dice [0.9257] 
2023-05-02 17:50:08.911463: Epoch time: 148.67 s 
2023-05-02 17:50:16.423018:  
2023-05-02 17:50:17.629168: Epoch 75 
2023-05-02 17:50:21.683368: Current learning rate: 0.00287 
2023-05-02 17:52:48.619617: train_loss -0.9791 
2023-05-02 17:52:50.006127: val_loss -0.8632 
2023-05-02 17:52:52.639196: Pseudo dice [0.9254] 
2023-05-02 17:52:53.860239: Epoch time: 152.2 s 
2023-05-02 17:53:00.989739:  
2023-05-02 17:53:02.167254: Epoch 76 
2023-05-02 17:53:04.313760: Current learning rate: 0.00277 
2023-05-02 17:55:29.235344: train_loss -0.9792 
2023-05-02 17:55:30.497845: val_loss -0.8689 
2023-05-02 17:55:31.643728: Pseudo dice [0.9295] 
2023-05-02 17:55:33.910156: Epoch time: 148.25 s 
2023-05-02 17:55:39.665893:  
2023-05-02 17:55:40.904357: Epoch 77 
2023-05-02 17:55:42.254577: Current learning rate: 0.00266 
2023-05-02 17:58:10.892307: train_loss -0.9797 
2023-05-02 17:58:12.093688: val_loss -0.8523 
2023-05-02 17:58:13.403643: Pseudo dice [0.9221] 
2023-05-02 17:58:14.614576: Epoch time: 151.23 s 
2023-05-02 17:58:20.752963:  
2023-05-02 17:58:22.133453: Epoch 78 
2023-05-02 17:58:23.954243: Current learning rate: 0.00256 
2023-05-02 18:00:53.417598: train_loss -0.9797 
2023-05-02 18:00:54.786126: val_loss -0.8714 
2023-05-02 18:00:56.761976: Pseudo dice [0.9298] 
2023-05-02 18:00:58.876328: Epoch time: 152.67 s 
2023-05-02 18:01:00.945688: Yayy! New best EMA pseudo Dice: 0.9228 
2023-05-02 18:01:10.501067:  
2023-05-02 18:01:11.908523: Epoch 79 
2023-05-02 18:01:13.832852: Current learning rate: 0.00245 
2023-05-02 18:03:39.398162: train_loss -0.9796 
2023-05-02 18:03:40.793683: val_loss -0.8568 
2023-05-02 18:03:43.002082: Pseudo dice [0.9241] 
2023-05-02 18:03:44.299594: Epoch time: 148.9 s 
2023-05-02 18:03:51.101247: Yayy! New best EMA pseudo Dice: 0.9229 
2023-05-02 18:04:00.242280:  
2023-05-02 18:04:01.574546: Epoch 80 
2023-05-02 18:04:03.695303: Current learning rate: 0.00235 
2023-05-02 18:06:30.112687: train_loss -0.9796 
2023-05-02 18:06:31.366939: val_loss -0.8668 
2023-05-02 18:06:33.478531: Pseudo dice [0.9293] 
2023-05-02 18:06:35.919142: Epoch time: 149.87 s 
2023-05-02 18:06:37.122516: Yayy! New best EMA pseudo Dice: 0.9236 
2023-05-02 18:06:45.812137:  
2023-05-02 18:06:47.083859: Epoch 81 
2023-05-02 18:06:49.714215: Current learning rate: 0.00224 
2023-05-02 18:09:16.180488: train_loss -0.9796 
2023-05-02 18:09:17.507610: val_loss -0.8731 
2023-05-02 18:09:18.695591: Pseudo dice [0.9321] 
2023-05-02 18:09:22.184078: Epoch time: 150.37 s 
2023-05-02 18:09:24.411773: Yayy! New best EMA pseudo Dice: 0.9244 
2023-05-02 18:09:33.814242:  
2023-05-02 18:09:35.161216: Epoch 82 
2023-05-02 18:09:37.203337: Current learning rate: 0.00214 
2023-05-02 18:12:03.172963: train_loss -0.9795 
2023-05-02 18:12:04.573435: val_loss -0.8515 
2023-05-02 18:12:06.889720: Pseudo dice [0.9217] 
2023-05-02 18:12:09.299230: Epoch time: 149.36 s 
2023-05-02 18:12:14.139836:  
2023-05-02 18:12:15.408892: Epoch 83 
2023-05-02 18:12:17.943125: Current learning rate: 0.00203 
2023-05-02 18:14:42.683044: train_loss -0.9803 
2023-05-02 18:14:43.983639: val_loss -0.8638 
2023-05-02 18:14:45.985357: Pseudo dice [0.928] 
2023-05-02 18:14:48.164362: Epoch time: 148.54 s 
2023-05-02 18:14:50.862669: Yayy! New best EMA pseudo Dice: 0.9245 
2023-05-02 18:14:59.317372:  
2023-05-02 18:15:00.516410: Epoch 84 
2023-05-02 18:15:02.759537: Current learning rate: 0.00192 
2023-05-02 18:17:25.308612: train_loss -0.9801 
2023-05-02 18:17:26.670215: val_loss -0.8608 
2023-05-02 18:17:28.985050: Pseudo dice [0.925] 
2023-05-02 18:17:30.323045: Epoch time: 145.99 s 
2023-05-02 18:17:32.765071: Yayy! New best EMA pseudo Dice: 0.9246 
2023-05-02 18:17:41.526817:  
2023-05-02 18:17:42.913886: Epoch 85 
2023-05-02 18:17:46.381878: Current learning rate: 0.00181 
2023-05-02 18:20:09.179285: train_loss -0.9804 
2023-05-02 18:20:10.416672: val_loss -0.8549 
2023-05-02 18:20:11.616917: Pseudo dice [0.9248] 
2023-05-02 18:20:13.862836: Epoch time: 147.65 s 
2023-05-02 18:20:15.172099: Yayy! New best EMA pseudo Dice: 0.9246 
2023-05-02 18:20:24.397951:  
2023-05-02 18:20:25.558773: Epoch 86 
2023-05-02 18:20:26.845632: Current learning rate: 0.0017 
2023-05-02 18:22:50.555204: train_loss -0.9804 
2023-05-02 18:22:51.825811: val_loss -0.86 
2023-05-02 18:22:53.683780: Pseudo dice [0.9258] 
2023-05-02 18:22:56.263513: Epoch time: 146.16 s 
2023-05-02 18:22:58.566637: Yayy! New best EMA pseudo Dice: 0.9247 
2023-05-02 18:23:07.296608:  
2023-05-02 18:23:08.475941: Epoch 87 
2023-05-02 18:23:10.792658: Current learning rate: 0.00159 
2023-05-02 18:23:10.792658: Current learning rate: 0.00159 
2023-05-02 18:25:36.715653: train_loss -0.9805 
2023-05-02 18:25:38.042829: val_loss -0.8558 
2023-05-02 18:25:39.506657: Pseudo dice [0.9251] 
2023-05-02 18:25:41.801723: Epoch time: 149.42 s 
2023-05-02 18:25:44.011217: Yayy! New best EMA pseudo Dice: 0.9248 
2023-05-02 18:25:54.559616:  
2023-05-02 18:25:56.091221: Epoch 88 
2023-05-02 18:25:57.183650: Current learning rate: 0.00148 
2023-05-02 18:28:22.560416: train_loss -0.9804 
2023-05-02 18:28:23.814864: val_loss -0.8549 
2023-05-02 18:28:26.051252: Pseudo dice [0.9242] 
2023-05-02 18:28:28.302299: Epoch time: 148.0 s 
2023-05-02 18:28:33.781034:  
2023-05-02 18:28:34.921427: Epoch 89 
2023-05-02 18:28:36.108413: Current learning rate: 0.00137 
2023-05-02 18:31:00.526592: train_loss -0.9807 
2023-05-02 18:31:01.868031: val_loss -0.8554 
2023-05-02 18:31:03.114536: Pseudo dice [0.9232] 
2023-05-02 18:31:05.250258: Epoch time: 146.75 s 
2023-05-02 18:31:14.614845:  
2023-05-02 18:31:15.801685: Epoch 90 
2023-05-02 18:31:17.597593: Current learning rate: 0.00126 
2023-05-02 18:33:40.829345: train_loss -0.9809 
2023-05-02 18:33:42.204380: val_loss -0.8584 
2023-05-02 18:33:44.146224: Pseudo dice [0.9258] 
2023-05-02 18:33:45.349087: Epoch time: 146.22 s 
2023-05-02 18:33:51.413190:  
2023-05-02 18:33:52.589031: Epoch 91 
2023-05-02 18:33:55.047817: Current learning rate: 0.00115 
2023-05-02 18:36:19.055714: train_loss -0.9811 
2023-05-02 18:36:20.330667: val_loss -0.8616 
2023-05-02 18:36:21.725190: Pseudo dice [0.9282] 
2023-05-02 18:36:25.228578: Epoch time: 147.64 s 
2023-05-02 18:36:27.502820: Yayy! New best EMA pseudo Dice: 0.925 
2023-05-02 18:36:35.829090:  
2023-05-02 18:36:37.195749: Epoch 92 
2023-05-02 18:36:38.523944: Current learning rate: 0.00103 
2023-05-02 18:39:03.423083: train_loss -0.9813 
2023-05-02 18:39:04.788998: val_loss -0.8503 
2023-05-02 18:39:06.015939: Pseudo dice [0.9208] 
2023-05-02 18:39:07.306602: Epoch time: 147.6 s 
2023-05-02 18:39:13.381721:  
2023-05-02 18:39:14.719442: Epoch 93 
2023-05-02 18:39:14.719442: Epoch 93 
2023-05-02 18:39:19.484329: Current learning rate: 0.00091 
2023-05-02 18:41:43.422376: train_loss -0.9812 
2023-05-02 18:41:44.668040: val_loss -0.869 
2023-05-02 18:41:46.977931: Pseudo dice [0.9301] 
2023-05-02 18:41:49.169848: Epoch time: 150.04 s 
2023-05-02 18:41:50.405382: Yayy! New best EMA pseudo Dice: 0.9252 
2023-05-02 18:42:02.471615:  
2023-05-02 18:42:03.656308: Epoch 94 
2023-05-02 18:42:05.851874: Current learning rate: 0.00079 
2023-05-02 18:44:30.635996: train_loss -0.9814 
2023-05-02 18:44:32.545506: val_loss -0.8591 
2023-05-02 18:44:34.865588: Pseudo dice [0.9251] 
2023-05-02 18:44:35.992232: Epoch time: 148.17 s 
2023-05-02 18:44:41.498169:  
2023-05-02 18:44:42.969869: Epoch 95 
2023-05-02 18:44:45.377569: Current learning rate: 0.00067 
2023-05-02 18:47:11.393136: train_loss -0.9811 
2023-05-02 18:47:12.710444: val_loss -0.8522 
2023-05-02 18:47:14.027088: Pseudo dice [0.9227] 
2023-05-02 18:47:15.411018: Epoch time: 149.9 s 
2023-05-02 18:47:22.009460:  
2023-05-02 18:47:23.263677: Epoch 96 
2023-05-02 18:47:24.546444: Current learning rate: 0.00055 
2023-05-02 18:49:50.478967: train_loss -0.981 
2023-05-02 18:49:51.795744: val_loss -0.849 
2023-05-02 18:49:53.866381: Pseudo dice [0.9216] 
2023-05-02 18:49:56.408968: Epoch time: 148.47 s 
2023-05-02 18:50:01.105217:  
2023-05-02 18:50:02.246691: Epoch 97 
2023-05-02 18:50:04.680417: Current learning rate: 0.00043 
2023-05-02 18:52:30.076603: train_loss -0.9815 
2023-05-02 18:52:31.423277: val_loss -0.8544 
2023-05-02 18:52:32.722908: Pseudo dice [0.9246] 
2023-05-02 18:52:35.759351: Epoch time: 148.97 s 
2023-05-02 18:52:40.721383:  
2023-05-02 18:52:41.926481: Epoch 98 
2023-05-02 18:52:43.043951: Current learning rate: 0.0003 
2023-05-02 18:55:09.477540: train_loss -0.9813 
2023-05-02 18:55:10.823675: val_loss -0.8544 
2023-05-02 18:55:12.252911: Pseudo dice [0.9242] 
2023-05-02 18:55:13.438682: Epoch time: 148.76 s 
2023-05-02 18:55:19.240161:  
2023-05-02 18:55:20.586681: Epoch 99 
2023-05-02 18:55:21.868654: Current learning rate: 0.00016 
2023-05-02 18:57:47.219022: train_loss -0.9814 
2023-05-02 18:57:48.505142: val_loss -0.8618 
2023-05-02 18:57:50.508627: Pseudo dice [0.927] 
2023-05-02 18:57:52.515004: Epoch time: 147.98 s 
2023-05-02 18:58:02.249553: Training done. 
2023-05-02 18:58:04.604210: Using splits from existing split file: drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_Liver/splits_final.json 
2023-05-02 18:58:05.741842: The split file contains 5 splits. 
2023-05-02 18:58:07.907330: Desired fold for training: 2 
2023-05-02 18:58:09.130157: This split has 13 training and 3 validation cases. 
2023-05-02 18:58:11.370814: predicting 34 
2023-05-02 18:58:13.815611: predicting 5 
2023-05-02 18:58:16.731165: predicting 8 
2023-05-02 18:59:04.069858: Validation complete 
2023-05-02 18:59:05.725485: Mean Validation Dice:  0.9261174187778466 
